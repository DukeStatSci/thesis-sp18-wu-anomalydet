<!-- ## Abstract {-} -->
The goal of this paper is to identify novel methods for detecting anomalies in network IP data. The data is comprised of four continuous features (source bytes, destination bytes, source packets, destination packets) divided by their respective source port and destination port combinations. Thus, the data is represented as a 3-dimensional tensor $T \in \mathbb{R}^{m \ times n \times 4$, where $m$ is the number of source ports, $n$ is the number of destination ports, and $4$ is the number of continuous features. Each cell in $T$, $t_{ijk}$ stores the mean of the observations of continuous feature $k$ between source port at index $i$ and destination port at index $j$. This paper proposes three techniques for generating means to fill in the missing cells in $T$, thereby completing the tensor, so as to provide reasonable estimates for new observations between every possible port combination. In the context of anomaly detection, new observations between ports that do not align closely with their corresponding estimate in $T$ are considered anomalies. The first technique uses a low-rank singular value decomposition algorithm for completing individual matrix slices of the tensor. The second defines a statistical model for the values in $T$ and uses a Bayesian Gibbs Sampling Procedure to simulate missing values in individual matrix slices of $T$. Finally, the third approach extends the first and second approaches to completing the tensor all at once, rather than with completing individual matrices.

<!-- Chapters 3,4, and 5 discuss three methods for completing the tensor $T$. The first two techniques slice $T$ into four matrices divided by the four continuous features: $Y^{(1)}, Y^{(2)}, Y^{(3)}, Y^{(4)} \in \mathbb{R}^{m \times n}$. Because both techniques apply to each matrix separately, the techniques will refer to a general matrix $Y$, which represents any of $Y^{(1)}, Y^{(2)}, Y^{(3)}, Y^{(4)}$. Each $Y^{(k)}$ has missingness because not every source port interacts with every destination port. Chapter three considers an iterative approach using an alternating least squares technique and the best low-rank approximation of $Y$ to repeatedly calculate estimates for the missing values of the singular value decomposition of $Y$. While the approach does not consider the variable sample sizes and variances for each port combination, essentially treating each cell as a scalar value rather than a mean of observations, it is the fastest technique of the three and provides reasonable performance metrics. Chapter four shores up the weaknesses of chapter three by defining an additive statistical model that accounts for the variable sample sizes and variances of the observations in each port combination. This model is generalized to a weighted least squares problem, and a Bayesian approach is used to create a Gibbs Sampler to iteratively simulate the row factors and column factors with their respective variances of the model. Each approach is validated on simulated data where the ground truth is known to verify correctness before being applied to the actual networks dataset. Finally, chapter five proposes a tensor completion technique that simulates cells in $T$ without slicing the tensor. This approach allows considers correlation and collinearity between the different continuous features and relies on the PARAFAC tensor decomposition (as opposed to the two-dimensional matrix singular value decomposition).      -->