# Alternating Least Squares Applied to Two Dimensional Matrices

This approach determines the best low-rank approximation for $Y$ using the Eckart-Young-Mirsky Theorem to repeatedly generate the orthonormal matrices $U$ and $V$ of the singular value decomposition of $Y$ ($Y = UDV^T$) in an alternating pattern. The technique is proven to correctly determine a low rank approximation on simulated matrices where the true rank is known. Once the optimal low rank is determined from the actual dataset the optimal low rank is used in the technique to simulate the estimates for the missing cells of $Y$. The technique's fit is assessed by comparing the fitted versus the observed values 

## Related Work

A common approach to matrix completion revolves around the underlying assumption that there exists a low rank approximation for the data matrix, particularly in the case of high dimensional data. Hastie, Mazumder, Lee, and Zadeh (2014) devised a similar approach to the one presented in this chapter that fuses nuclear-norm-regularized matrix approximation (Candes and Tao, 2009, Mazumder, Hastie and Tibshirani, 2010) and maximum-margin matrix factorization (Srebro, Rennie and Jaakkola, 2005), resulting in a fast alternating least squares that relies on a low rank singular value decomposition to drive an efficient algorithm for large matrix factorization. 
 
Similar techniques for matrix completion were employed heavily in the Netflix Challenge where competitors predicted ratings for movies by users that had not watched the movie based on the other ratings in the matrix of users and movies. The winning team, BellKorâ€™s Pragmatic Chaos, employed a low rank decomposition technique to reduce the incredibly large dataset, so that other algorithms could be applied without too much computational overhead (Andreas Toscher,  Michael Jahrer, Robert M. Bell 2009). 

Network datasets can range up to billions of observations (recall this particular dataset with 1 million observations was collected in just five minutes). Furthermore, there are up to 65000 possible network source and destination ports, so the resulting network tensor has large dimensions. Given the nature of the dataset and prior work in matrix completion, the technique in this chapter assumes a low rank decomposition to implement an alternating least squares completion technique.

## Matrix Completion Algorithm

$F \in \mathbb{R}^{m \times n}$ is a sparse matrix that represents the frequencies of combinations, i.e $F[32242,12312]$ represents the number of observations for the 32242 12312 port combination $M \in \mathbb{R}^{m \times n}$ represents a boolean matrix of whether the corresponding $Y$ values are missing. $Y[M]$ represents all of the missing values in $Y$.

The objective is $$\underset{r}{\text{min}} \sum_{i,j:F_{i,j} > 0} (y_{i,j} - u_iDv^T_j)^2$$ where $UDV^{(k)T}$ represents the singular value decomposition of $Y$ and $r$ is the low rank approximation for $Y$. There are multiple steps to the matrix completion process:

### ANOVA Initial Imputation
An analysis of variance (ANOVA) imputation is used to fill in the initial values for $y_{ij}$. This yields an additive model dependent upon the means of the present observations: $$y_{ij} =  a_i + b_j - \mu$$ where$\mu$ is the overall mean of $Y$, $a_i$ is the row mean, and $b_j$ is column mean of $y_{ij}$.

### Repeated Simulation
The repeated imputation procedure solves $Y^{(s)}[M] = R_r(Y^{(s-1)})[M]$ where $R_r( ... )$ is the best rank $r$ approximation for the $s$-th step. For each step $(s)$ the singular value decomposition decomposes $$Y^{(s)} =  U^{(s)}D^{(s)}V^{T(s)}$$ where $D$ is a diagonal matrix of the singular values, $U$ is the left singular vectors of $Y$ and $V$ is the right singular vectors of $Y$. 

The Eckart-Young-Mirsky Theorem provides the best rank $r$ approximation for the missing values in $Y^{(s+1)}$. Recall $Y[M]$ represents all of the missing values of $Y$. Applying the EYM theorem:  $$Y^{(s+1)}[M] = (U[,1:r]^{(s)}D[,1:r]^{(s)}V[,1:r]^{T(s)})[M]$$ Where $U[,1:r]$ represents the first $r$ columns of $U$ and the same for $D$ and $V$. 

### Convergence Criterion

The Eckart-Young-Mirsky rank approximation step is repeated until the relative difference between $Y^{(s+1)}$ and $Y^{(s)}$ falls below a set threshold, $H$. The relative difference threshold is expressed: $$\frac{\|Y^{(s+1)}-Y^{(s)}\|_2}{\|Y^{(s)}\|_2} < H$$ where $\|Y\|_2$ is the Frobenius norm for matrices. The denominator of the expression ensures the convergence criterion is invariate to a scale change in the matrix itself. 

```{r,include=FALSE}
Y = readRDS("data/means_SB.rds")
M = readRDS("data/freqs.rds")
# ## REMOVING FULL MISSING COLUMNS FROM MATRICES
# Y = Y[, colSums(is.na(Y))!=nrow(Y)] #remove NA cols
# Y = Y[rowSums(is.na(Y)) != ncol(Y),] #remove NA rows
# ## REMOVING FULL 0 COLUMNS FROM MATRICES
# M = M[ , !apply(M==0,2,all)]
# M = M[ !apply(M==0,1,all) , ]
####Eckhart Young Theorem Implementation, Best Rank k Approximation####
matrix_complete = function(S = 1000, k = 2, nrows, ncols, Y, M){
  Y_imputed = Y
  #overall mean
  n = sum(M)
  mu = sum(Y, na.rm = TRUE)/n
  #calculate row means and col means
  a_i = rowMeans(Y, na.rm = TRUE)
  b_j = colMeans(Y, na.rm = TRUE)
  #set NaN to 0 in means to fix anova fill in
  a_i = sapply(a_i, function(x) if (!is.finite(x)) {0} else {x})
  b_j = sapply(b_j, function(x) if (!is.finite(x)) {0} else {x})
  #Fill in missing values in Y_imputed with ANOVA
  #Y_imputed = outer(1:nrow(Y), 1:ncol(Y), function(r,c) ifelse(M[r,c] == 0, a_i[r] + b_j[c] - mu, Y[r,c]))
  for (i in 1:nrows){
    for (j in 1:ncols){
      if (M[i,j] == 0){
        Y_imputed[i,j] = a_i[i] + b_j[j] - mu
      }
    }
  }
  for (s in 1:S){
    #extract SVD
    svd_Y = svd(Y_imputed)
    U = svd_Y$u
    V = svd_Y$v
    #EYM theorem
    if (k == 1){
      EYM = (matrix(U[,1:k]) * (svd_Y$d)[1:k]) %*% t(matrix(V[,1:k]))
    }
    else {
      EYM = U[,1:k] %*% diag((svd_Y$d)[1:k]) %*% t(V[,1:k])
    }
    for (i in 1:nrows){
      for (j in 1:ncols){
        if (M[i,j] == 0){
          Y_imputed[i,j] = EYM[i,j]
        }
      }
    }
  }
  return (Y_imputed)
}
```

## Best Low Rank Approximation

To determine the best low rank for approximating $Y$, Leave-One-Out Cross Validation (LOOCV) is used to generate prediction errors for each possible rank. LOOCV cycles through the observed values, setting each to NA (missing), and then performing the described matrix completion process. The prediction error is then calculated as some function of the difference between the imputed value and the true value. In this case, the algorithm records root mean square error $$\sqrt{\frac{\sum (\hat y_{ij} - y_{ij})^2}{z}}$$ where $z$ is the number of observations not missing.

```{r,include=FALSE}
#Leave One Out Cross Validation
loocv = function (S = 1000, k = 2, nrows = nrows, ncols = ncols, Y, M){
  error = 0
  rmse = 0
  n = 0
  for (i in 1:nrows){
    for (j in 1:ncols){
      if (M[i,j] != 0){
        n = n + 1
        M_imputed = M
        true_sd = Y[i,j]
        M_imputed[i,j] = 0
        Y_imputed = matrix_complete(S, k, nrows, ncols, Y, M_imputed)
        error = error + abs((Y_imputed[i,j] - Y[i,j]))
        rmse = rmse + (Y_imputed[i,j] - Y[i,j])^2
      }
    }
  }
  rmse = sqrt(rmse/n)
  return (list(Error = error, RMSE = rmse, Observations = n))
}
```

## Validation Against Simulated Data

Before applying the algorithm on the real data it is useful to validate the algorithmic approach against simulated data where the true rank is already known. 

### Simulating a Low Rank Matrix 

Taking the Kronecker product of two lower dimension matrices yields a higher dimension matrix with low rank. Explicitly, given matrix $A \in \mathbb{R}^{m \times r}$ and $B \in \mathbb{R}^{r \times n}$, $A \otimes B = C$ where $C \in \mathbb{R}^{m \times n}$ has rank $r$. Thus, when $r < m, r < n$ the matrix $C$ has an optimal low rank that minimizes the root mean square error from the leave one out cross validation procedure. To add noise to the simulated matrix, $C$, simply add an error matrix, $E \in \mathbb{R}^{m \times n}$ sampled from a normal distribution.

This procedure provides a computationally efficient way to simulate many random low rank matrices to use as inputs for the validation procedure. In the case of simulated matrices, there are no missing entries, so the leave one out cross validation procedure sequentially removes each cell in the matrix, imputes its value using the rank being investigated, and considers the individual cell error as the difference between the true value and the imputed value. The overall root mean square error for the technique is then calculated with the aggregate each of these individual cell errors.

```{r,include=FALSE}
# generate m x n matrix with rank r, add noise
generate_low_rank_matrix = function(m, n, r, noise = FALSE){
  A = matrix(rnorm(m * r, mean=0, sd=1), m, r) 
  B = matrix(rnorm(r * n, mean=0, sd=1), r, n)
  AB = A %*% B
  if (noise){
    E = matrix(rnorm(m * n, mean=0, sd=1), m, n)
    AB = AB + E
  }
  return (AB)
}
```

### Approximating Optimal Rank

```{r, include=FALSE}

# sets each elements to missing one at a time
full_loocv = function (S = 1000, k, nrows, ncols, Y){
  error = 0
  rmse = 0
  n = 0
  for (i in 1:nrows){
    for (j in 1:ncols){
      n = n + 1
      M = Y
      M[i,j] = 0
      Y_imputed = matrix_complete(S, k, nrows, ncols, Y, M)
      error = error + abs((Y_imputed[i,j] - Y[i,j]))
      rmse = rmse + (Y_imputed[i,j] - Y[i,j])^2
    }
  }
  rmse = sqrt(rmse/n)
  return (list(Error = error, RMSE = rmse, Observations = n))
}

approximate_rank = function(Y, M, S = 50, simulated = TRUE){
  nrows = nrow(Y)
  ncols = ncol(Y)
  ranks = c()
  all_errors = list()
  for (i in 1:S){
    cat("Approximation Round: ", i, '\n')
    if (simulated){
      cv_errors = lapply(seq(1,10,1), function(k) full_loocv(200,k,nrows,ncols,Y)$RMSE)
    }
    else{
      cv_errors = lapply(seq(1,10,1), function(k) loocv(200,k,nrows,ncols,Y, M)$RMSE)
    }
    low_rank = which.min(cv_errors)
    ranks = c(ranks, low_rank)
  }
  return (ranks)
}
```

```{r, echo=FALSE}
Ranks = c(1,2,3,4,5,6,7,8)
Accuracy = c(1,1,1,1,0.8,0.8,0.6,0.5)
Ranks = c(1,2,3,4,5,6,7,8)
Accuracy_Noise = c(0.9,1,0.9,1,0.9,0.7,0.5,0.4)
par(mfrow=c(1,2))
plot(Ranks, Accuracy, main = "No Noise", 
     xlab = "Rank (r)", ylab = "Accuracy")
plot(Ranks, Accuracy_Noise, main = "Additional Noise", 
     xlab = "Rank (r)", ylab = "Accuracy")
```

The above plots represent the accuracy of the matrix completion technique for matrices with true rank $r = 1,2,...8$. The accuracy is measured by simulating 10 random matrices (dimensions ranging from 74 to 100 values) with low rank of $r$ for each value of $r$ (80 simulated matrices total), and then running the leave one out cross validation procedure described above on the matrix $C$ to generate a root mean square error for each possible rank. The accuracy is the calculated using the number of times the rank with the lowest error matches the true simulated rank divided by 100 (the number of trials with rank $r$). Note as the true rank becomes larger, the technique performs far worse at determining the true rank. This behavior is due to the fact that LOOCV attempts to find the rank that minimizes the root mean square error, not necessarily the true low rank approximation for a matrix. Higher true ranks tend to give higher out-of-sample validation error, so LOOCV will still select a low rank approximation for simulated matrices that have relatively high true ranks. For instance, a simulated data matrix may have a true rank of 8, but it may also be very close to rank 2, which results in LOOCV selecting rank 2 as the optimal low rank approximation for minimizing error.

Furthermore, when noise is applied to each simulated matrix, $C$, (through the addition of a noise matrix $E$), the algorithm tends to perform worse at a majority of the attempted ranks. This is expected because the addition of noise to every cell in the matrix may obscure the true rank from the LOOCV procedure.

## Results on Real Data

```{r,echo=FALSE}
# RMSEs = lapply(seq(1,8,1), function(k) loocv(250,k, nrow(Y), ncol(Y), Y, M)$RMSE)
# [[1]]
# [1] 42727.19
# 
# [[2]]
# [1] 31878.69
# 
# [[3]]
# [1] 44605.58
# 
# [[4]]
# [1] 48193.36
# 
# [[5]]
# [1] 46903.49
# 
# [[6]]
# [1] 47509.22
# 
# [[7]]
# [1] 47807.57
# 
# [[8]]
# [1] 48136.36
rmses = c( 42727.19, 33187.69, 44605.58, 48193.36, 46903.49, 47509.22, 47807.57, 48136.36 )
plot (seq(1,8,1), rmses, xlab = "Matrix Rank", ylab = "Root Mean Square Errors",
      main = "Leave One Out Cross Validation for Matrix Ranks")
```

The above plot displays the root mean square errors from the leave one out cross validation process across different rank inputs into the algorithm. It's clear that rank 2 provides the best low-rank approximation for simulating missing values in $Y$ using the alternating least squares algorithm. Thus, the dataset is fitted with the algorithm using rank 2 to simulate the missing values in $Y$. 

```{r, echo = FALSE}
### version of EYM that replaces the considers the entire fitted matrix
matrix_complete = function(S = 1000, k = 2, nrows, ncols, Y, M){
  Y_imputed = Y
  #overall mean
  n = sum(M)
  mu = sum(Y, na.rm = TRUE)/n
  #calculate row means and col means
  a_i = rowMeans(Y, na.rm = TRUE)
  b_j = colMeans(Y, na.rm = TRUE)
  #set NaN to 0 in means to fix anova fill in
  a_i = sapply(a_i, function(x) if (!is.finite(x)) {0} else {x})
  b_j = sapply(b_j, function(x) if (!is.finite(x)) {0} else {x})
  #Fill in missing values in Y_imputed with ANOVA
  #Y_imputed = outer(1:nrow(Y), 1:ncol(Y), function(r,c) ifelse(M[r,c] == 0, a_i[r] + b_j[c] - mu, Y[r,c]))
  for (i in 1:nrows){
    for (j in 1:ncols){
      if (M[i,j] == 0){
        Y_imputed[i,j] = a_i[i] + b_j[j] - mu
      }
    }
  }
  for (s in 1:S){
    #extract SVD
    svd_Y = svd(Y_imputed)
    U = svd_Y$u
    V = svd_Y$v
    #EYM theorem
    if (k == 1){
      EYM = (matrix(U[,1:k]) * (svd_Y$d)[1:k]) %*% t(matrix(V[,1:k]))
    }
    else {
      EYM = U[,1:k] %*% diag((svd_Y$d)[1:k]) %*% t(V[,1:k])
    }
    Y_imputed = EYM
  }
  return (Y_imputed)
}
Y_imputed = matrix_complete(250, 2, nrow(Y), ncol(Y), Y, M)
par(mfrow = c(1,2))
plot(Y[!is.na(Y)], Y_imputed[!is.na(Y)], xlab = "Observed Values of Y", 
     ylab = "Fitted Values of Y", main = "Observed vs Fitted Values Y")
abline(0,1, col = "red")
plot(Y[!is.na(Y)], Y_imputed[!is.na(Y)], xlim = c(0,1000), ylim = c(0,1000),
     xlab = "Observed Values of Y (max 1000)", ylab = "Fitted Values of Y (max 1000)", 
     main = "Observed vs Fitted Values Y")
abline(0,1, col = "red")
```

The two plots above display the true values of the $Y$ matrix (i.e. the non-missing values) versus their corresponding fitted values using the alternating least squares algorithm with an input of rank 2. The first plot displays all values and shows a somewhat positive linear trend (an ideal fit of the true values would be a scatter of points following a linear relationship, represented in red). However, several outliers with large true values skew this dataset and cause the plot to appear linear. Closer examination of the true and fitted values smaller than 1000 (the plot on the right) reveals the relationship is far from the linear pattern.

### Scale Transformations

The poorly fitted results motivates a consideration of the scale of the data. The present algorithm uses the sample averages of the overall matrix as well as the row and column means when imputing each missing cell value. This reliance upon sample means leads to susceptiblity to outliers. Moreover exploratory data analysis reveals the dataset contains outliers, particularly in the SrcBytes and DstBytes measurements. Because outliers drive the sum of squares for the alternating least squares procedure, the poor fit on the data is unsurprising. Thus, a transformation of features in the dataset may be appropriate for improving the fit of the algorithm.

When a natural logarithm transformation is applied to the raw dataset before any simulation steps are taken, the alternating least squares imputation algorithm yields the following root mean square errors varied by rank.

```{r, echo = FALSE}
Y = readRDS("data/log_src_byte_Y.rds")
# > RMSEs = lapply(seq(1,8,1), function(k) loocv(250,k, nrow(Y), ncol(Y), Y, M)$RMSE)
# > RMSEs
# [[1]]
# [1] 1.444161
# 
# [[2]]
# [1] 1.913014
# 
# [[3]]
# [1] 2.249744
# 
# [[4]]
# [1] 2.285776
# 
# [[5]]
# [1] 2.483833
# 
# [[6]]
# [1] 2.72687
# 
# [[7]]
# [1] 3.167227
# 
# [[8]]
# [1] 3.541428
rmses_log = c(1.444161,1.913014, 2.249744, 2.285776, 
              2.483833, 2.72687,3.167227, 3.541428)
plot (seq(1,8,1), rmses_log)
```

Now the optimal rank from the LOOCV procedure is 1. The algorithm is run on the log transformed dataset with a low rank approximation of 1 and the fitted vereus observed values are again compared.

```{r, echo = FALSE}
Y_imputed = matrix_complete(250, 1, nrow(Y), ncol(Y), Y, M)
par(mfrow = c(1,2))
exp_Y = exp(Y)
exp_Y_imp = exp(Y_imputed)
plot(exp_Y[!is.na(exp_Y)], exp_Y_imp[!is.na(exp_Y)])
abline(0,1, col = "red")
plot(exp_Y[!is.na(exp_Y)], exp_Y_imp[!is.na(exp_Y)], 
     xlim = c(0,1000), ylim = c(0,1000))
abline(0,1, col = "red")
```

The values above have been retransformed (exponentiated) to the dataset's original scale after the simulation procedure was completed. The fit still appears to be quite poor and there is not much difference between the log transformed output versus the original non-transformed output.

This poor performance may largely be due to the fact the algorithm does not account for the variability in the sample size and variance in the observed interactions for each cell. Unlike the Netflix Competition, in which each cell of the matrix being completed contained only a single user rating of a movie, the matrix in this problem contains the mean of a variable number of observations corresponding to particular port combinations.




