# Alternating Least Squares Applied to Two Dimensional Matrices

## Motivation 

The preliminary tensor imputation technique slices the tensor into four separate matrices, $Y^{(1)}, Y^{(2)}, Y^{(3)}, Y^{(4)} \in \mathbb{R}_{m \times n}$. Each matrix has dimensions defined by the number of source ports by the number of destination ports. Each cell, $Y^{(k)}_{ij}$ contains the mean of the observations observed for continuous feature $k$ and source port $i$ destination port $j$. Each matrix will have its missing cells imputed with information from the rest of that matrix, and separately from information about the other matrices. The four completed matrices will then be recombined to create the tensor $Y$. Similar techniques for matrix completion were employed in the Netflix Challenge where top competitor predicted ratings for movies by users that had not watched the movie based on the other ratings in the matrix of users and movies.

## Matrix Completion Algorithm

Each $Y^{(k)}$ has missingness because not every source port interacts with every destination port. $F \in {\rm I\!R}^{m \times n}$ is a sparse matrix that represents the frequencies of combinations, i.e $F[32242,12312]$ represents the number of observations for the 32242 12312 port interaction. $M \in {\rm I\!R}^{m \times n}$ represents a boolean matrix of whether the corresponding $Y$ values are missing. $Y^{(k)}[M]$ represents all of the missing values of $Y^{(k)}$. Moreover because each non-missing observation contains all four continuous features, $Y[M]$ represents all the missing values of $Y$.

The objective is $$min \sum_{i,j:F_{i,j} > 0} (Y^{(k)}_{i,j} - u^{(k)}_iD^{(k)}v^{(k)T}_j)^2$$ where $U^{(k)}D^{(k)}V^{(k)T}$ represents the singular value decomposition of $Y^{(k)}$. There are multiple steps to the matrix completion process:

### Anova Initial Imputation

Impute the initial values for the missing $y^{(k)}_{i,j}$ observations $1 \leq i \leq m, 1 \leq j \leq n$: In general an additive model is applicable: $$y^{(k)}_{i,j} = \mu^{(k)} + a^{(k)}_i + b^{(k)}_j + \epsilon^{(k)}_{i,j}$$ where $\epsilon^{(k)} \in N(0,\sigma^2)$, $\mu^{(k)}$ is the overall mean of $Y^{(k)}$, $a^{(k)}_i$ is the row mean, and $b^{(k)}_j$ is column mean. An analysis of variance (ANOVA) imputation is used to fill in the initial values, $y^{(k)}_{i,j}$. Ignoring the missing values for now, let $y_{..}$ denote the empirical overall mean, $y_{i.}$ denote the empirical row mean, and $y_{.j}$ denote the column mean. $$y_{i,j} = y_{..} + (y_{i.}-y{..}) + (y_{.j}-y_{..}) = y_{i.} + y_{.j} - y{..}$$

### Repeated Imputation

The repeated imputation procedure solves $Y^{(s)}[M] = R_k(Y^{(s-1)})[M]$ where $R_k$ is the best rank-k approximation for the $s$-th step. For each step $(s)$ use singular value decomposition to decompose $$Y^{(s)} =  U^{(s)}DV^{T(s)}$$ where $D$ is a diagonal matrix of the singular values, $U$ is the left singular vectors of $Y$ and $V$ is the right singular vectors of $Y$. 

The Eckart-Young-Mirsky (EYM) Theorem provides the best rank-k approximation for the missing values in $Y^{(s+1)}$. Recall $Y[M]$ represents all of the missing values of $Y$. Applying the EYM theorem:  $$Y^{(s+1)}[M] = (U[,1:k]^{(s)}D[,1:k]V[,1:k]^{T(s)})[M]$$ Where $U[,1:k]$ represents the first $k$ columns of $U$ and the same for $D$ and $V$. 

### Convergence Criterion

The EYM rank approximation imputation steps are repeated until the relative difference between $Y^{(s+1)}$ and $Y^{(s)}$ falls below a set threshold, $T$. The relative difference threshold is expressed: $$\frac{\|Y^{(s+1)}-Y^{(s)}\|_2}{\|Y^{(s)}\|_2} < T$$ where $\|Y\|_2$ is the Frobenius norm. The denominator of the expression ensures the convergence criterion is invariate to a scale change in the matrix itself. 

### Implementation

```{r,include=FALSE}
setwd("~/Desktop/Stats Thesis/thesis-sp18-wu-anomalydet/")
Y = readRDS("data/means.RDS")
M = readRDS("data/freqs.RDS")
## REMOVING FULL MISSING COLUMNS FROM MATRICES
Y = Y[, colSums(is.na(Y))!=nrow(Y)] #remove NA cols
Y = Y[rowSums(is.na(Y)) != ncol(Y),] #remove NA rows
## REMOVING FULL 0 COLUMNS FROM MATRICES
M = M[ , !apply(M==0,2,all)]
M = M[ !apply(M==0,1,all) , ]
####Eckhart Young Theorem Implementation, Best Rank k Approximation####
matrix_complete = function(S = 1000, k = 2, nrows, ncols, Y, M){
  Y_imputed = Y
  #overall mean
  n = sum(M)
  mu = sum(Y, na.rm = TRUE)/n
  #calculate row means and col means
  a_i = rowMeans(Y, na.rm = TRUE)
  b_j = colMeans(Y, na.rm = TRUE)
  #set NaN to 0 in means to fix anova fill in
  a_i = sapply(a_i, function(x) if (!is.finite(x)) {0} else {x})
  b_j = sapply(b_j, function(x) if (!is.finite(x)) {0} else {x})
  #Fill in missing values in Y_imputed with ANOVA
  #Y_imputed = outer(1:nrow(Y), 1:ncol(Y), function(r,c) ifelse(M[r,c] == 0, a_i[r] + b_j[c] - mu, Y[r,c]))
  for (i in 1:nrows){
    for (j in 1:ncols){
      if (M[i,j] == 0){
        Y_imputed[i,j] = a_i[i] + b_j[j] - mu
      }
    }
  }
  for (s in 1:S){
    #extract SVD
    svd_Y = svd(Y_imputed)
    U = svd_Y$u
    V = svd_Y$v
    #EYM theorem
    if (k == 1){
      EYM = (matrix(U[,1:k]) * (svd_Y$d)[1:k]) %*% t(matrix(V[,1:k]))
    }
    else {
      EYM = U[,1:k] %*% diag((svd_Y$d)[1:k]) %*% t(V[,1:k])
    }
    for (i in 1:nrows){
      for (j in 1:ncols){
        if (M[i,j] == 0){
          Y_imputed[i,j] = EYM[i,j]
        }
      }
    }
  }
  return (Y_imputed)
}
```

## Assessing Imputation Strategy

### Leave One Out Cross Validation

To assess the quality of the imputation, Leave-One-Out Cross Validation (LOOCV) is used to generate a prediction error. LOOCV cycles through the observed values, setting each to NA (missing), and then performing the described imputation process. The prediction error is then calculated as some function of the difference between the imputed value and the true value. In this case, the algorithm records absolute error $\sum \mid \hat y_{i,j} - y_{i,j}\mid$ and root mean square error $\sqrt{\frac{\sum (\hat y_{i,j} - y_{i,j})^2}{n}}$ where $n$ is the number of observations not missing.

### Implementation

```{r,include=FALSE}
#Leave One Out Cross Validation
loocv = function (S = 1000, k = 2, nrows = nrows, ncols = ncols, Y, M){
  error = 0
  rmse = 0
  n = 0
  for (i in 1:nrows){
    for (j in 1:ncols){
      if (M[i,j] != 0){
        n = n + 1
        M_imputed = M
        true_sd = Y[i,j]
        M_imputed[i,j] = 0
        Y_imputed = matrix_complete(S, k, nrows, ncols, Y, M_imputed)
        error = error + abs((Y_imputed[i,j] - Y[i,j]))
        rmse = rmse + (Y_imputed[i,j] - Y[i,j])^2
      }
    }
  }
  rmse = sqrt(rmse/n)
  return (list(Error = error, RMSE = rmse, Observations = n))
}
```

### Results

```{r,eval = FALSE,echo=FALSE}
RMSEs = lapply(seq(1,8,1), function(k) loocv(250,k, nrow(Y), ncol(Y), Y, M)$RMSE)
plot(seq(1,8,1), RMSEs)
```

While matrix completion via singular value decomposition presents valid missing value imputations, and the algorithm converges relatively quickly, the error generated from leave one out cross validation reflects that the imputation performs rather poorly for low rank solutions to the data. Moreover, the errors are minimized at a rank approximation of 3, but even at this rank, the errors are relatively high considering the data was first normal transformed. 

This poor performance may largely be due to the fact the algorithm does not account for the variability in the number of observed solutions for each cell being imputed. Unlike the Netflix Competition, in which each cell of the matrix being completed contained only a single user rating of a movie, the matrix in this problem contains the average of a variable number of observations in each cell.

## Validation Against Simulated Data

### Simulating a Low Rank Matrix 

```{r,include=FALSE}
# generate m x n matrix with rank r, add noise
generate_low_rank_matrix = function(m, n, r, noise = FALSE){
  A = matrix(rnorm(m * r, mean=0, sd=1), m, r) 
  B = matrix(rnorm(r * n, mean=0, sd=1), r, n)
  AB = A %*% B
  if (noise){
    E = matrix(rnorm(m * n, mean=0, sd=1), m, n)
    AB = AB + E
  }
  return (AB)
}
```

### Approximating Optimal Rank

```{r, include=FALSE}

# sets each elements to missing one at a time
full_loocv = function (S = 1000, k, nrows, ncols, Y){
  error = 0
  rmse = 0
  n = 0
  for (i in 1:nrows){
    for (j in 1:ncols){
      n = n + 1
      M = Y
      M[i,j] = 0
      Y_imputed = matrix_complete(S, k, nrows, ncols, Y, M)
      error = error + abs((Y_imputed[i,j] - Y[i,j]))
      rmse = rmse + (Y_imputed[i,j] - Y[i,j])^2
    }
  }
  rmse = sqrt(rmse/n)
  return (list(Error = error, RMSE = rmse, Observations = n))
}

approximate_rank = function(Y, M, S = 50, simulated = TRUE){
  nrows = nrow(Y)
  ncols = ncol(Y)
  ranks = c()
  all_errors = list()
  for (i in 1:S){
    cat("Approximation Round: ", i, '\n')
    if (simulated){
      cv_errors = lapply(seq(1,10,1), function(k) full_loocv(200,k,nrows,ncols,Y)$RMSE)
    }
    else{
      cv_errors = lapply(seq(1,10,1), function(k) loocv(200,k,nrows,ncols,Y, M)$RMSE)
    }
    low_rank = which.min(cv_errors)
    ranks = c(ranks, low_rank)
  }
  return (ranks)
}
```