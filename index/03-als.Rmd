# Alternating Least Squares Applied to Two Dimensional Matrices

## Motivation 

The preliminary tensor imputation technique slices the tensor into four separate matrices, $Y^{(1)}, Y^{(2)}, Y^{(3)}, Y^{(4)} \in \mathbb{R}_{m \times n}$. Each matrix has dimensions defined by the number of source ports by the number of destination ports. Each cell, $Y^{(k)}_{ij}$ contains the mean of the observations observed for continuous feature $k$ and source port $i$ destination port $j$. Each matrix will have its missing cells imputed with information from the rest of that matrix, and separately from information about the other matrices. The four completed matrices will then be recombined to create the tensor $Y$. Similar techniques for matrix completion were employed in the Netflix Challenge where top competitor predicted ratings for movies by users that had not watched the movie based on the other ratings in the matrix of users and movies.

## Matrix Completion Algorithm

Each $Y^{(k)}$ has missingness because not every source port interacts with every destination port. $F \in \mathbb{R}_{m \times n}$ is a sparse matrix that represents the frequencies of combinations, i.e $F[32242,12312]$ represents the number of observations for the 32242 12312 port interaction. $M \in \mathbb{R}_{m \times n}$ represents a boolean matrix of whether the corresponding $Y$ values are missing. $Y^{(k)}[M]$ represents all of the missing values of $Y^{(k)}$. Moreover because each non-missing observation contains all four continuous features, $Y[M]$ represents all the missing values of $Y$.

The objective is $$min \sum_{i,j:F_{i,j} > 0} (Y^{(k)}_{i,j} - u^{(k)}_iD^{(k)}v^{(k)T}_j)^2$$ where $U^{(k)}D^{(k)}V^{(k)T}$ represents the singular value decomposition of $Y^{(k)}$. There are multiple steps to the matrix completion process:

### ANOVA Initial Imputation

Impute the initial values for the missing $y^{(k)}_{i,j}$ observations $1 \leq i \leq m, 1 \leq j \leq n$: In general an additive model is applicable: $$y^{(k)}_{i,j} = \mu^{(k)} + a^{(k)}_i + b^{(k)}_j + \epsilon^{(k)}_{i,j}$$ where $\epsilon^{(k)} \in N(0,\sigma^2)$, $\mu^{(k)}$ is the overall mean of $Y^{(k)}$, $a^{(k)}_i$ is the row mean, and $b^{(k)}_j$ is column mean. An analysis of variance (ANOVA) imputation is used to fill in the initial values, $y^{(k)}_{i,j}$. Ignoring the missing values for now, let $y_{..}$ denote the empirical overall mean, $y_{i.}$ denote the empirical row mean, and $y_{.j}$ denote the column mean. $$y_{i,j} = y_{..} + (y_{i.}-y{..}) + (y_{.j}-y_{..}) = y_{i.} + y_{.j} - y{..}$$

### Repeated Imputation

The repeated imputation procedure solves $Y^{(s)}[M] = R_k(Y^{(s-1)})[M]$ where $R_k$ is the best rank-k approximation for the $s$-th step. For each step $(s)$ use singular value decomposition to decompose $$Y^{(s)} =  U^{(s)}DV^{T(s)}$$ where $D$ is a diagonal matrix of the singular values, $U$ is the left singular vectors of $Y$ and $V$ is the right singular vectors of $Y$. 

The Eckart-Young-Mirsky (EYM) Theorem provides the best rank-k approximation for the missing values in $Y^{(s+1)}$. Recall $Y[M]$ represents all of the missing values of $Y$. Applying the EYM theorem:  $$Y^{(s+1)}[M] = (U[,1:k]^{(s)}D[,1:k]V[,1:k]^{T(s)})[M]$$ Where $U[,1:k]$ represents the first $k$ columns of $U$ and the same for $D$ and $V$. 

### Convergence Criterion

The EYM rank approximation imputation steps are repeated until the relative difference between $Y^{(s+1)}$ and $Y^{(s)}$ falls below a set threshold, $T$. The relative difference threshold is expressed: $$\frac{\|Y^{(s+1)}-Y^{(s)}\|_2}{\|Y^{(s)}\|_2} < T$$ where $\|Y\|_2$ is the Frobenius norm. The denominator of the expression ensures the convergence criterion is invariate to a scale change in the matrix itself. 

```{r,include=FALSE}
Y = readRDS("data/means_SB.rds")
M = readRDS("data/freqs.rds")
# ## REMOVING FULL MISSING COLUMNS FROM MATRICES
# Y = Y[, colSums(is.na(Y))!=nrow(Y)] #remove NA cols
# Y = Y[rowSums(is.na(Y)) != ncol(Y),] #remove NA rows
# ## REMOVING FULL 0 COLUMNS FROM MATRICES
# M = M[ , !apply(M==0,2,all)]
# M = M[ !apply(M==0,1,all) , ]
####Eckhart Young Theorem Implementation, Best Rank k Approximation####
matrix_complete = function(S = 1000, k = 2, nrows, ncols, Y, M){
  Y_imputed = Y
  #overall mean
  n = sum(M)
  mu = sum(Y, na.rm = TRUE)/n
  #calculate row means and col means
  a_i = rowMeans(Y, na.rm = TRUE)
  b_j = colMeans(Y, na.rm = TRUE)
  #set NaN to 0 in means to fix anova fill in
  a_i = sapply(a_i, function(x) if (!is.finite(x)) {0} else {x})
  b_j = sapply(b_j, function(x) if (!is.finite(x)) {0} else {x})
  #Fill in missing values in Y_imputed with ANOVA
  #Y_imputed = outer(1:nrow(Y), 1:ncol(Y), function(r,c) ifelse(M[r,c] == 0, a_i[r] + b_j[c] - mu, Y[r,c]))
  for (i in 1:nrows){
    for (j in 1:ncols){
      if (M[i,j] == 0){
        Y_imputed[i,j] = a_i[i] + b_j[j] - mu
      }
    }
  }
  for (s in 1:S){
    #extract SVD
    svd_Y = svd(Y_imputed)
    U = svd_Y$u
    V = svd_Y$v
    #EYM theorem
    if (k == 1){
      EYM = (matrix(U[,1:k]) * (svd_Y$d)[1:k]) %*% t(matrix(V[,1:k]))
    }
    else {
      EYM = U[,1:k] %*% diag((svd_Y$d)[1:k]) %*% t(V[,1:k])
    }
    for (i in 1:nrows){
      for (j in 1:ncols){
        if (M[i,j] == 0){
          Y_imputed[i,j] = EYM[i,j]
        }
      }
    }
  }
  return (Y_imputed)
}
```

### Leave One Out Cross Validation

To assess the quality of the imputation, Leave-One-Out Cross Validation (LOOCV) is used to generate a prediction error. LOOCV cycles through the observed values, setting each to NA (missing), and then performing the described imputation process. The prediction error is then calculated as some function of the difference between the imputed value and the true value. In this case, the algorithm records absolute error $\sum \mid \hat y_{i,j} - y_{i,j}\mid$ and root mean square error $\sqrt{\frac{\sum (\hat y_{i,j} - y_{i,j})^2}{n}}$ where $n$ is the number of observations not missing.

```{r,include=FALSE}
#Leave One Out Cross Validation
loocv = function (S = 1000, k = 2, nrows = nrows, ncols = ncols, Y, M){
  error = 0
  rmse = 0
  n = 0
  for (i in 1:nrows){
    for (j in 1:ncols){
      if (M[i,j] != 0){
        n = n + 1
        M_imputed = M
        true_sd = Y[i,j]
        M_imputed[i,j] = 0
        Y_imputed = matrix_complete(S, k, nrows, ncols, Y, M_imputed)
        error = error + abs((Y_imputed[i,j] - Y[i,j]))
        rmse = rmse + (Y_imputed[i,j] - Y[i,j])^2
      }
    }
  }
  rmse = sqrt(rmse/n)
  return (list(Error = error, RMSE = rmse, Observations = n))
}
```

## Validation Against Simulated Data

Before applying the algorithm on the real data it is useful to validate the algorithmic approach against simulated data where the true rank is known. 

### Simulating a Low Rank Matrix 

Taking the Kronecker product of two lower dimension matrices yields a higher dimension matrix with low rank. Explicitly, given matrix $A \in \mathbb{R}_{m \times r}$ and $B \in \mathbb{R}_{r \times n}$, $A \otimes B = C$ where $C \in \mathbb{R}_{m \times n}$ with rank $r$. Thus, when $r < m, r < n$ the matrix $C$ has an optimal low rank that minimizes the root mean square error from the leave one out cross validation procedure. To add noise to the simulated matrix, $C$, simply add an error matrix, $E \in \mathbb{R}$ where $e_{i,j} \sim N(0,1)$.

Moreover, this procedure provides a computationally efficient way to simulate many random low rank matrices to use as inputs for the validation procedure. In the case of simulated matrices, there are no missing entries, so the leave one out cross validation procedure sequentially removes each cell in the matrix, imputes its value using the rank being investigated, and considers the individual cell error as the difference between the true value and the imputed value. The overall root mean square error for the technique is then calculated with the aggregate each of these individual cell errors.

```{r,include=FALSE}
# generate m x n matrix with rank r, add noise
generate_low_rank_matrix = function(m, n, r, noise = FALSE){
  A = matrix(rnorm(m * r, mean=0, sd=1), m, r) 
  B = matrix(rnorm(r * n, mean=0, sd=1), r, n)
  AB = A %*% B
  if (noise){
    E = matrix(rnorm(m * n, mean=0, sd=1), m, n)
    AB = AB + E
  }
  return (AB)
}
```

### Approximating Optimal Rank

```{r, include=FALSE}

# sets each elements to missing one at a time
full_loocv = function (S = 1000, k, nrows, ncols, Y){
  error = 0
  rmse = 0
  n = 0
  for (i in 1:nrows){
    for (j in 1:ncols){
      n = n + 1
      M = Y
      M[i,j] = 0
      Y_imputed = matrix_complete(S, k, nrows, ncols, Y, M)
      error = error + abs((Y_imputed[i,j] - Y[i,j]))
      rmse = rmse + (Y_imputed[i,j] - Y[i,j])^2
    }
  }
  rmse = sqrt(rmse/n)
  return (list(Error = error, RMSE = rmse, Observations = n))
}

approximate_rank = function(Y, M, S = 50, simulated = TRUE){
  nrows = nrow(Y)
  ncols = ncol(Y)
  ranks = c()
  all_errors = list()
  for (i in 1:S){
    cat("Approximation Round: ", i, '\n')
    if (simulated){
      cv_errors = lapply(seq(1,10,1), function(k) full_loocv(200,k,nrows,ncols,Y)$RMSE)
    }
    else{
      cv_errors = lapply(seq(1,10,1), function(k) loocv(200,k,nrows,ncols,Y, M)$RMSE)
    }
    low_rank = which.min(cv_errors)
    ranks = c(ranks, low_rank)
  }
  return (ranks)
}
```

WHAT PLOTS OR VISUALIZATIONS DO I USE FOR RESULTS? HISTOGRAM OF PERCENTAGES FOR RANK APPROXIMATIONS? 1-4 100%, higher ranks arent that relevant; effects of noise

USE THIS TECHNIQUE TO SHOW DIFFERENT EFFECTS OF NOISE? 

The above plots represent the accuracy of the matrix completion technique for matrices with true rank $k = 1,2,...8$. The accuracy is measured by simulating 10 random matrices with low rank of $k$ for each value of $k$ (80 simulated matrices total), and then running the leave one out cross validation procedure described above on the matrix $C$ to generate a root mean square error for each possible rank. The accuracy is the calculated using the number of times the rank with the lowest error matches the true simulated rank divided by 10. Note as the true rank becomes larger, the technique performs far worse at determining the optimal low rank, THIS IS BECAUSE_______

When a noise matrix $E$ is added to each simulated matrix $C$


## Results on Real Data

```{r,echo=FALSE}
# RMSEs = lapply(seq(1,8,1), function(k) loocv(250,k, nrow(Y), ncol(Y), Y, M)$RMSE)
# [[1]]
# [1] 42727.19
# 
# [[2]]
# [1] 31878.69
# 
# [[3]]
# [1] 44605.58
# 
# [[4]]
# [1] 48193.36
# 
# [[5]]
# [1] 46903.49
# 
# [[6]]
# [1] 47509.22
# 
# [[7]]
# [1] 47807.57
# 
# [[8]]
# [1] 48136.36
rmses = c( 42727.19, 33187.69, 44605.58, 48193.36, 46903.49, 47509.22, 47807.57, 48136.36 )
plot (seq(1,8,1), rmses)
```

The above plot displays the root mean square errors from leave one out cross validation across different rank inputs into the algorithm. It's clear that rank 2 provides the best low-rank solution for the alternating least squares imputation algorithm. Thus rank 2 is used in the algorithm to impute the missing values in $Y$. 

```{r, echo = FALSE}
### version of EYM that replaces the considers the entire fitted matrix
matrix_complete = function(S = 1000, k = 2, nrows, ncols, Y, M){
  Y_imputed = Y
  #overall mean
  n = sum(M)
  mu = sum(Y, na.rm = TRUE)/n
  #calculate row means and col means
  a_i = rowMeans(Y, na.rm = TRUE)
  b_j = colMeans(Y, na.rm = TRUE)
  #set NaN to 0 in means to fix anova fill in
  a_i = sapply(a_i, function(x) if (!is.finite(x)) {0} else {x})
  b_j = sapply(b_j, function(x) if (!is.finite(x)) {0} else {x})
  #Fill in missing values in Y_imputed with ANOVA
  #Y_imputed = outer(1:nrow(Y), 1:ncol(Y), function(r,c) ifelse(M[r,c] == 0, a_i[r] + b_j[c] - mu, Y[r,c]))
  for (i in 1:nrows){
    for (j in 1:ncols){
      if (M[i,j] == 0){
        Y_imputed[i,j] = a_i[i] + b_j[j] - mu
      }
    }
  }
  for (s in 1:S){
    #extract SVD
    svd_Y = svd(Y_imputed)
    U = svd_Y$u
    V = svd_Y$v
    #EYM theorem
    if (k == 1){
      EYM = (matrix(U[,1:k]) * (svd_Y$d)[1:k]) %*% t(matrix(V[,1:k]))
    }
    else {
      EYM = U[,1:k] %*% diag((svd_Y$d)[1:k]) %*% t(V[,1:k])
    }
    Y_imputed = EYM
  }
  return (Y_imputed)
}
Y_imputed = matrix_complete(250, 2, nrow(Y), ncol(Y), Y, M)
par(mfrow = c(1,2))
plot(Y[!is.na(Y)], Y_imputed[!is.na(Y)])
abline(0,1, col = "red")
plot(Y[!is.na(Y)], Y_imputed[!is.na(Y)], xlim = c(0,1000), ylim = c(0,1000))
abline(0,1, col = "red")
```

The two plots above display the true values of the $Y$ matrix (i.e. the non-missing values) versus their corresponding fitted values using the alternating least squares algorithm with an input of rank 2. The first plot displays all values and shows a somewhat positive linear trend (an ideal fit of the true values would be a scatter of points following the 45 degree angled line represented in red). However, several outliers with large true values skew this dataset and cause the plot to appear linear. Closer examination of the true and fitted values smaller than 1000 (the plot on the right) reveals the relationship is far from the linear pattern.


DISCUSS THE NEED TO HAVE DATA WHERE THE ENTIRE ROW OR COLUMN CANT BE MISSING

### Scale Transformations

The poorly fitted results motivates a consideration of the scale of the data. The present algorithm uses the sample averages of the overall matrix as well as the row and column means when imputing each missing cell value. This reliance upon sample means leads to susceptiblity to outliers. Moreover exploratory data analysis reveals the dataset contains outliers, particularly in the SrcBytes and DstBytes measurements. Thus, a transformation of features in the model may be appropriate for improving the fit of the algorithm.

OUTLIERS DRIVE SUM OF SQUARES

When a natural log transformation is applied to the raw dataset before any imputation steps are taken, the alternating least squares imputation algorithm yields the following root mean square errors varied by rank.

```{r, echo = FALSE}
Y = readRDS("data/log_src_byte_Y.rds")
# > RMSEs = lapply(seq(1,8,1), function(k) loocv(250,k, nrow(Y), ncol(Y), Y, M)$RMSE)
# > RMSEs
# [[1]]
# [1] 1.444161
# 
# [[2]]
# [1] 1.913014
# 
# [[3]]
# [1] 2.249744
# 
# [[4]]
# [1] 2.285776
# 
# [[5]]
# [1] 2.483833
# 
# [[6]]
# [1] 2.72687
# 
# [[7]]
# [1] 3.167227
# 
# [[8]]
# [1] 3.541428
rmses_log = c(1.444161,1.913014, 2.249744, 2.285776, 
              2.483833, 2.72687,3.167227, 3.541428)
plot (seq(1,8,1), rmses_log)
```

```{r, echo = FALSE}
Y_imputed = matrix_complete(250, 1, nrow(Y), ncol(Y), Y, M)
par(mfrow = c(1,2))
exp_Y = exp(Y)
exp_Y_imp = exp(Y_imputed)
plot(exp_Y[!is.na(exp_Y)], exp_Y_imp[!is.na(exp_Y)])
abline(0,1, col = "red")
plot(exp_Y[!is.na(exp_Y)], exp_Y_imp[!is.na(exp_Y)], 
     xlim = c(0,1000), ylim = c(0,1000))
abline(0,1, col = "red")
```



<!-- While matrix completion via singular value decomposition presents valid missing value imputations, and the algorithm converges relatively quickly, the error generated from leave one out cross validation reflects that the imputation performs rather poorly for low rank solutions to the data. Moreover, the errors are minimized at a rank approximation of 3, but even at this rank, the errors are relatively high considering the data was first normal transformed.  -->

<!-- This poor performance may largely be due to the fact the algorithm does not account for the variability in the number of observed solutions for each cell being imputed. Unlike the Netflix Competition, in which each cell of the matrix being completed contained only a single user rating of a movie, the matrix in this problem contains the average of a variable number of observations in each cell. -->

