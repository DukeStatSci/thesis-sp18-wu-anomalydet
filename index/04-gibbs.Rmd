# A Bayesian Approach to Matrix Completion

The previous section's results reflected the need for a completion strategy that accounts for the variability in the number of observations observed for each port combination when imputing that particular combination's cell. The previous technique fails to take into account the differing sample size and variance in each cell's observations, so the algorithm treated each $y_{ij}$ as a single value rather than the mean and variance of a vector of observations. The following section constructs a statistical model that takes the sample size of observations and their variances for each cell into account and repeatedly simulates values for the missing cells using a Gibbs Sampling procedure. The sampling procedure relies upon first building a general model for simulating the row and column facors with their respective standard deviations. After calculating the full conditionals for the parameters of this general model, the overall procedure repeatedly simulates values from these full conditional distributions, alternating between simulating the matrix of row factors and the matrix of column factors along with their respective standard deviations. Note, this technique again slices the tensor into the four separate matrices, $Y^{(1)}, Y^{(2)}, Y^{(3)}, Y^{(4)} \in \mathbb{R}^{m \times n}$ (referred to as $Y$ in general), and the model can be applied to each matrix $Y^{(k)}$ separately.

## Related Work

Bayesian methods are becoming increasingly popular as a matrix completion technique for large scale datasets. Work by Zhou, Wang, Chen, Paisley, Dunson and Carin (2010) indicate Gibbs Sampling provides an efficient solution to large scale problems and yields "predictions as well as a measure of confidence in each prediction." Their paper considers algorithm performance in several datasets of varying scale and relationship between variables, and the results indicate strong performance compared to other common approaches. Granted, this approach considers non-parametric Bayesian matrix completion, while the Gibbs Sampler in this chapter  relies upon constructing the full conditional distributions for parameters in a defined statistical model. Nevertheless, the hypothesis remains that Gibbs Sampling provides an efficient and effective solution for matrix completion. Mai and Alquier also support this claim in their paper "A Bayesian Approach for Noisy Matrix Completion: Optimal Rate under General Sampling Distribution" (2014), in which they construct a Bayesian estimator that relies upon the premise that "Bayesian methods for low-rank matrix completion with noise have been shown to be very efficient computationally." They apply this technique to the Netflix competition dataset as a case study.

## Statistical Model for Port Relationships

<!-- Additive Main Effects and Multiplicative Interaction Models (AMMI models) provide a defined statistical model for each cell in the ports matrix. In particular, the model combines the additive effects of the initial ANOVA imputation with the multiplicative effects yielded from singular value decomposition described in the previous section. More importantly, the model also includes a variance term for each cell that takes into account the differing frequency of observations in each port combination.  -->

The following statistical model is defined for the cells in $Y$: $$y_{ij} = u_i^Tv_j + \frac{\sigma_i \tau_j}{\sqrt{s_{ij}}}\epsilon_{ij}$$ where $u_i$ represents the row factors, $v_j$ represents the column factors, $\sigma_i$ represents the standard deviation of each row in the matrix, $\tau_j$ represents the standard deviations of each column in the matrix, $s_{ij}$ represents the sample size of observations observed for source port $i$ and destination port $j$, and $\epsilon_{ij} \sim N(0,1)$. Fixing the $j$ values in the analysis (i.e. $v_j$ and $\tau_j$ are known) enables the model to be rewritten in the form of a weighted least squares model for estimating $u_i$ and $\sigma_i$. Similarly, when $i$ is fixed, the model can be rewritten to estimate $v_j$ and $\tau_j$. To demonstrate this property, the procedure for estimating $u_i$ and $\tau_j$ given known values for $v_j$ and $\tau_j$ is described below. The same procedure is possible for $v_j$ and $\tau_j$ when $u_i$ and $\sigma_i$ are known.

## General Linear Model for Simulating Row and Column Factors

Varying $j = 1 ... n$ the model above yields the following cell values: 
$$y_{i1} = u_i^Tv_1 + \frac{\sigma_i \tau_1}{\sqrt{s_{i1}}}\epsilon_{i1}$$ 
$$ ... $$ 

$$ y_{in} = u_i^Tv_n + \frac{\sigma_i \tau_n}{\sqrt{s_{in}}}\epsilon_{in} $$

Vectorizing all of these equations varied across $j = 1...n$ yields:

$$\vec{y_i} = Vu_i + \sigma_i W^{1/2}\vec{\epsilon}$$ 

<!-- $$\vec{y_i} = \beta^Tx_i + \sigma w_i^{1/2}\epsilon_{ij}$$  -->
<!-- where $w_i = (\frac{\tau_j}{\sqrt(n_{ij}})^2$, and $\beta^T = u_i$. -->
<!-- Vectorizing the above model yields $$\vec{y} = X\beta + \sigma W^{1/2} \epsilon$$  -->
where $V \in \mathbb{R}^{n \times p}$ is the matrix of column factors ($p$ is the dimension of the latent factors), and $W \in \mathbb{R}^{n \times n}$ is the diagonal matrix of weights, such that
$$V =
  \begin{bmatrix}
    v_1^T- \\
    v_2^T- \\
    ... \\
    v_n^T- \\
  \end{bmatrix},
  W =
  \begin{bmatrix}
    w_{1} & & \\
    & \ddots & \\
    & & w_{n}
  \end{bmatrix} 
  = \begin{bmatrix}
    \frac{\tau_1^2}{s_{11}} & & \\
    & \ddots & \\
    & & \frac{\tau_n^2}{s_{nn}}
  \end{bmatrix}$$

Note $\tau^2$ refers to the variance, variance being the square of the standard deviation.

This model can be rewritten in a general form: 

$$\vec{y} = X\beta + \sigma W^{1/2}\epsilon$$ where $X$ represents $V$, $\beta$ represents $u_i$ and $\sigma$ represents $\sigma_i$.

This is a modified form of the Generalized Least Squares Model (GLS), which gives a weighted least squares estimate of $\beta$, and it is appropriate when the error terms are not independent and identically distributed. Bayesian analysis of this problem provides similar parameter estimates to GLS, and both ordinary least squares and GLS provide unbiased parameter estimates of $\beta$ with the latter giving estimates with a lower variance because the non-Bayes estimator serves as a limit of the Bayes estimator. 

The full conditional distributions of the random variables $\beta$ and $\sigma^2$ (note $\sigma$ is squared in the model) for this case are described below: 
$$\{\beta \mid X, \vec{y}, \sigma^2\} \sim MVN (\beta_n, \Sigma_n)$$
$$\{\sigma^2 \mid X, \vec{y}, \beta\} \sim IG (\frac{\nu_0 + n}{2}, \frac{v_0\sigma^2_0 + SSR_W}{2})$$
where MVN represents the Multivariate Normal Distribution, and IG represents the Inverse Gamma distribution.
$$ \begin{cases}
      \Sigma_n = (X^TW^{-1}X/\sigma^2+\Sigma_0^{-1})^{-1}\\
      \beta_n = \Sigma_n(X^TW^{-1}y/\sigma^2 + \Sigma_0^{-1} \beta_0)
    \end{cases}$$
$$SSR_W = (y - X\beta)^TW^{-1}(y-X\beta)$$

The formulation for the closed form full conditional distributions for the $\beta$ and $\sigma^2$ parameters are based upon the general full conditionals established for a regression model with correlated errors (Hoff 2009). These particular full conditional formulations are a special case of this model where $W$ is a diagonal matrix and so the covariance matrix is diagonal. This general regression model formulation also conveniently specifies the prior distributions.

The remaining variables in the closed form full conditionals come from the parameter's prior distributions, which are defined as follows: 

$$\beta \sim MVN (\beta_0, \Sigma_0)$$
$$\sigma^2 \sim IG (\frac{\nu_0}{2}, \frac{v_0}{2}\sigma_0^2)$$

The initial values for the prior distributions are set as: $\beta_0 = 0$, $\Sigma_0 = \gamma^2I$ where $\gamma^2$ is a large number and $I$ is the $m \times n$ identity matrix, $\nu_0 = 2$, $\sigma_0^2 = 1$. This results in a diffuse prior for $\beta$ that spreads out the density, and a noninformative prior for $\sigma^2$. 

Work by Alquier, Cottet, Chopin, Rousseau (2014) reveal that a standard approach to assigning priors in Bayesian Matrix completion "is to assign an inverse gamma prior to the singular values of a certain singular value decomposition of the matrix of interest; this prior is conjugate. However, [they] show that two other types of priors (again for the singular values) may be conjugate for this model: a gamma prior, and a discrete prior. Conjugacy is very convenient, as it makes it possible to implement either Gibbs sampling or Variational Bayes." In the case of this problem, the distributions of the priors are defined to be diffuse ($\beta$) and noninformative ($\sigma^2$), so that the effects of the priors on the posteriors are limited  when compared to the effects of the observed data.

### Gibbs sampler for the General Linear Model

Following the formulation of the model and the definition of the priors, a general Gibbs sampler function is created to simulate samples from the full conditional of each parameter in the statistical model, which iteratively creates an approximate value for each cell.

The Gibbs sampler algorithm progresses as follows:

Let the parameters at step $s$ be:

$\phi^{(s)} = \{\beta^{(s)}, \sigma^{2(s)}\}$

Sample $\beta^{(s+1)} \sim P(\beta \mid X, \vec{y}, \sigma^{2(k)})$

Sample $\sigma^2 \sim P(\sigma^2 \mid X, \vec{y}, \beta^{(s+1)})$

Set $\phi^{(s+1)} = \{\beta^{(s+1)}, \sigma^{2(k+1)}\}$

This Gibbs sampler serves as a general technique that can be used to simulate both the values of $u_i$ and $\sigma_i$ or $v_j$ and $\tau_j$ depending on the inputs it is given because the formulation of both models are identical; they only differ by the the inputs, $X$ and $W$, which are calculated, and $y$ which is sliced directly from $Y$. In the context of the problem, this function can first be called repeatedly to simulate all of the rows in the matrix $Y$, then called repeatedly with updated inputs to simulate all of the columns of the matrix. 

### Validation on Simulated Data

Before using the general Gibbs sampler function in the overall procedure for simulating missing values in $Y$, it is necessary to validate the procedure's effectiveness on simulated data where the ground truth is known. As the algorithm runs, it stores a matrix of $\beta$ vectors and a vector of $\sigma^2$ scalars. Thus, if $S = 50$, i.e. the algorithm samples 50 $\beta$ and 50 $\sigma$, the final returned output will be 
$$\beta =
  \begin{bmatrix}
    \beta_1^T- \\
    \beta_2^T- \\
    ... \\
    \beta_{50}^T- \\
  \end{bmatrix},
  \vec{\sigma^2} =
  \begin{bmatrix}
    \sigma_1^2 \\
    . \\
    . \\
    \sigma_{50}^2\\
  \end{bmatrix}$$

Using random sampled values from the normal distribution for $X$ and random sampled values from the exponential distribution for $W$ (exponential distribution is used to ensure $\Sigma_n$ is positive definite), it is possible to calculate values of $\vec{y}$ using a predefined $\beta*$ and $\sigma *$, which are known as the ground truth values for comparison: $$\vec{y} = X\beta * + \sigma * W^{1/2}\epsilon$$ 
This $\vec{y}$, $W$, and $X$ are used as inputs to the general Gibbs Sampler Function to generate a distribution of $\beta$'s and a distribution $\sigma^2$'s. The posterior means of these distributions are then computed and compared to recover the original values, $\beta *$ and $\sigma *$. 

```{r, eval = FALSE, include=FALSE}
p<-5
beta<-rnorm(p)
X = matrix(rnorm(m * p, mean=0, sd=1), m, p)
W = diag(x = rexp(m), nrow = m, ncol = m)
y = X%*%beta  + matrix(rnorm(m * 1, mean=0, sd=1), m, 1)

plot(beta, lm(y~ -1+ X)$coef)
abline(0,1)
```

In particular, the posterior mean of $\sigma^2$ is calculated by taking the mean of the function's output of $\sigma^2$. This posterior mean is compared to the original $\sigma *$ used to generate $\vec{y}$. Repeatedly performing this procedure reveals the posterior mean only differs from the ground truth value by 1-2% in almost every single trial.

Recovering the original $\beta *$ provides a much more defined procedure for evaluating the performance of the Gibbs Sampler. First, the Bayes estimator (the posterior mean of generated $\beta$s) should be close to the GLS estimator and theoretical results state the GLS estimator serves as a good approximation for the true value. Furthermore, the variance matrix of the GLS estimator around the true value is $$Var(\hat{\beta}_{GLS}) = \mathbb{E}[(\hat{\beta}_{GLS}-\beta * )(\hat{\beta}_{GLS}-\beta *)^T]=(X^T W^{-1}X/\sigma^2 )^{-1}$$
Thus, after simulating many data sets and solving for the posterior mean estimator, $\hat{\beta}$, the variance of these simulated posterior means, $Var(\hat{\beta})$ should be close to $(X^T W^{-1}X/\sigma^2 )^{-1}$. Moreover, the standard errors are calculated $$SE(\hat{\beta}_{GLS}) = \sqrt{diag((X^T W^{-1}X/\sigma^2 )^{-1})}$$ This provides a nominal 95% confidence interval for which to assess the performance of the model for recovering the original $\beta *$. 
<!-- The simulation procedure was able to recover a large percentage of the $\beta *$ values that resulted -->

<!-- run the algorithm more than 10 times, check that the beta posterior mean (column means of the beta matrix object) -->
<!-- get a distribution of betas, calculate the posterior mean of those, get a distribution of sigma2 (make sure it is, dont use variance of 1) -->

## Full Sampling Procedure

The complete sampling procedure for imputing missing values uses the generalized Gibbs Sampler defined above to iteratively simulate missing values for the entire matrix $Y$. The procedure is described below:

Initialize $\sigma_i$ for $i = 1 ... m$ and $\tau_j$ for $j = 1 ... n$ as the overall standard deviation of the $Y^{(k)}$ matrix. Initialize missing values of $Y$ using the ANOVA imputation described in the previous section. Initialize the matrix of row factors $U \in \mathbb{R}^{m \times p}$ and the matrix of column factors $V \in \mathbb{R}^{n \times p}$.  

Repeat the following:

1. For $i = 1 ... m$: Estimate $u_i$ and $\sigma_i$ using the generalized Gibbs Sampler. For the first iteration of the sampler, set $X$ to the $V$ matrix in the singular value decomposition of $Y$ (truncated to be $n \times p$). For all future iterations, the use the stored $V$ from the previous iteration as $X$. For the first iteration, use the initialized $\tau_j$ for $j = 1 ... n$ to calculate the diagonals for the $W$ matrix. For all future iterations use the stored $\tau_j$ values from the previous iteratioWn to calculate $W$. Take the corresponding $y_i$ directly from the $Y$ matrix. Store the resulting sampled $u_i$'s as rows of a matrix $U$, and the $\sigma_i$ in a vector to use for simulating $v_j$ and $\tau_j$. 

2. For $j = 1 ... n$: Estimate $v_j$ and $\tau_j$ using the generalized Gibbs Sampler. Use the stored $U$ from the previous step as the $X$ input and use the stored $\tau_j$ to calculate the $W$ input. Take the corresponding $y_j$ directly from the $Y$ matrix. Store the resulting sampled $v_j$'s in a matrix $V$, and the $\tau_j$ in a vector, to use for simulating $u_i$ and $\sigma_j$. 

3. Estimate values for $y_{ij}$ in $Y$ that were missing in the original dataset by sampling from the normal distribution $$y_{ij} \sim N(u_i^Tv_j, \frac{\sigma_i^2\tau_j^2}{s_{ij}})$$

### Selecting the Dimension of Latent Factors

The dimension of the latent factors, $p$, is used to define the dimension of $\beta \in \mathbb{R}^{p \times 1}$ and consequently defines the dimensions of the row and column factor matrices $U$ and $V$. Selecting $p$ is a model selection choice similar to determining the optimal low rank approximation $r$ in the previous section. Once again, Leave One Out Cross Validation may be used to detemine the optimal $p$ given the observed data. In this technique, it is more computationally expensive than the previous technique to perform Leave One Out Cross Validation on the entire dataset, so K-Fold cross validation or randomly selecting a set number of observed cells to set to missing for determining $p$ is also valid.

<!-- ### Validation on Simulated Data -->

<!-- Again it is necessary to validate the effectiveness of the overall sampling procedure with simulated data where the ground truth is known. Simulate matrices $U$ and $V$ such that $\Theta = UV^T$ is truly low rank. The same procedure for generating low rank matrices used in the previous chapter can be applied here. Using $\Theta$ there are two possible tests for determining the validity of the sampling procedure. First, the simulated data is used to recover the true values for $UV^T$ from the model. This is considered an idealized case for testing the procedure. Second, some cells are set to missing and again the procedure is used to obtain an estimate for $UV^T$ from the model. This output is compared to the previous case's output to evaluate the performance depending on whether the cells have data or do not.  -->

## Results on Real Data

```{r,echo=FALSE}
Y = readRDS("data/means_SB.rds")
Y_imputed = readRDS("data/Y_imputed.rds")
Y_imputed[Y_imputed < 0] = 0
# Y_cap = Y
# Y_cap[Y > 600000] = 600000
# Y_cap_imputed = Y_imputed
# Y_cap_imputed[Y_imputed > 600000] = 600000
# logY_imputed = log(Y_imputed)
# logY_imputed[logY_imputed < 0] = 0
levelplot(Y,scales=list(x=list(at=NULL),y=list(at=NULL)),
               main="SrcByte Means",xlab="SrcPorts",ylab="DstPorts")
```
```{r,echo=FALSE}
levelplot(Y_imputed, scales=list(x=list(at=NULL),y=list(at=NULL)),
               main="Estimated SrcByte Means",xlab="SrcPorts",ylab="DstPorts")
```

The above plots show the original matrix (left) and the completed matrix (right) using the full estimation procedure. The white cells in the left plot indicate missingness. The estimated means all fall within the range of the existing means (approximately 50000-400000). There exist several apparent patterns in the rows and columns of the estimated matrix, but these patterns are most likely the result of the full estimation procedure estimating $U$ (matrix of row factors), $V$ (matrix of column factors), then the missing values of $Y$. Because the model is made up of the product row and column effects, a large row effect is propagated throughout all entries in a row and similarly for column effects, resulting in the checkerboard pattern displayed. In terms of the networks data domain, it makes sense for certain ports to have more traffic (so the entire row/column is darker in the estimated matrix).

### Scale Transformations

Like in ALS, large outlier values in the row and column means may skew the row effects and column effects for this Gibbs sampling procedure, so the entire procedure is performed again with the initial values in the dataset log transformed. The below plots represent the results of feeding the log transformed dataset into the full estimation procedure and then exponentiating the  completed matrix to return it to the same scale as the original data.

```{r,echo=FALSE}
Y = readRDS("data/means_SB.rds")
Y_imputed = readRDS("data/exp_Y_imputed.rds")
# Y_cap = Y
# Y_cap[Y > 600000] = 600000
# Y_cap_imputed = Y_imputed
# Y_cap_imputed[Y_imputed > 600000] = 600000
# logY_imputed = log(Y_imputed)
# logY_imputed[logY_imputed < 0] = 0
levelplot(Y,scales=list(x=list(at=NULL),y=list(at=NULL)),
               main="SrcByte Means",xlab="SrcPorts",ylab="DstPorts")
```
```{r,echo=FALSE}
levelplot(Y_imputed, scales=list(x=list(at=NULL),y=list(at=NULL)),
               main="Estimated SrcByte Means (Log Transformed)",xlab="SrcPorts",ylab="DstPorts")

```

Taking the log transform reduces the effects of the large row and column means on the estimation procedure for the row and column effects. Thus, the row and column effects are less pronounced, resulting in the checkerboard pattern being muted in the results on the right. The sampled once again fall within a reasonable range of the existing means (approximately 50000-400000). 

The fit still stands to improve using improved model selection, but in this case achieving the optimal sampling procedure is limited by available computational resources. For instance, using cross validation to select the optimal dimension of latent factors, $p$, would likely improve the fit of the model. However, running LOOCV using the sampling technique is computationally expensive. Moreover, increasing the overall number of iterations of the full estimation procedure (it is currently at $S = 100$) may also improve the model fit. Work done by Raftery and Lewis (1992) suggests that "reasonable accuracy [with a Gibbs Sampler] may often be achieved with 5,000 iterations or less; this can frequently be reduced to less than 1,000 if the posterior tails are known to be light." 

