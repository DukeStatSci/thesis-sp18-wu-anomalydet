---
author: 'James C. Wu'
date: 'May 2018'
institution: 'Duke University'
division: 'Trinity College of Arts and Sciences'
advisor: 'Peter D. Hoff'

committeememberone: 'Jerry P. Reiter'
committeemembertwo: 'Galen Reeves'
dus: 'Mine Cetinkaya-Rundel'
department: 'Department of Statistical Science'
degree: 'Bachelor of Science in Statistical Science'
title: 'Matrix Completion Techniques For Anomaly Detection in Network Attacks'
knit: "bookdown::render_book"
site: bookdown::bookdown_site
output: 
  thesisdowndss::thesis_pdf: default
  thesisdowndss::thesis_gitbook: default
#  thesisdowndss::thesis_word: default
#  thesisdowndss::thesis_epub: default
# If you are creating a PDF you'll need to write your preliminary content here or
# use code similar to line 20 for the files.  If you are producing in a different
# format than PDF, you can delete or ignore lines 20-31 in this YAML header.
abstract: |
  `r if(knitr:::is_latex_output()) paste(readLines("00-abstract.Rmd"), collapse = '\n  ')`
# If you'd rather include the preliminary content in files instead of inline
# like below, use a command like that for the abstract above.  Note that a tab is
# needed on the line after the |.
acknowledgements: |
  I thank my advisor, Professor Peter Hoff, and the Director of Undergraduate Studies, Professor Mine Cetinkaya-Rundel, for their guidance in this project. I also thank Duke University's Statistics Department for supervising this project and the Office of Information Technology for providing the dataset. Most of all I thank my parents for their continued unwavering support in all my endeavors.
# dedication: |
#   You can have a dedication here if you wish.
# preface: |
#   This is an example of a thesis setup to use the reed thesis document class
#   (for LaTeX) and the R bookdown package, in general.
bibliography: bib/thesis.bib
# Refer to your specific bibliography file in the line above.
csl: csl/apa.csl
# Download your specific csl file and refer to it in the line above.
# lot: true #TABLES
# lof: true #FIGURES
space_between_paragraphs: true
# Delete the # at the beginning of the previous line if you'd like
# to have a blank new line between each paragraph
#header-includes:
#- \usepackage{tikz}
---
<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of metadata used to produce the document.  Be careful with spacing in this header!

If you'd prefer to not include a Dedication, for example, simply delete lines 17 and 18 above or add a # before them to comment them out.  If you have other LaTeX packages you would like to include, delete the # before header-includes and list the packages after hyphens on new lines.

If you'd like to include a comment that won't be produced in your resulting file enclose it in a block like this.
-->

<!--
If you receive a duplicate label error after knitting, make sure to delete the index.Rmd file and then knit again.
-->

```{r include_packages, include = FALSE}
# This chunk ensures that the thesisdowndss package is
# installed and loaded. This thesisdowndss package includes
# the template files for the thesis.
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(thesisdowndss))
  devtools::install_github("mine-cetinkaya-rundel/thesisdowndss")
library(thesisdowndss)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!--chapter:end:index.Rmd-->

<!-- ## Abstract {-} -->
The goal of this project is to identify novel methods for detecting anomalies in network IP data. The space is represented as a 3-dimensional tensor of the continuous features (source bytes, destination bytes, source packets, destination packets) divided by their respective source port and destination port combinations. This project implements and assesses the validity of principal component analysis and matrix completion via singular value decomposition (more methods pending) in determining anomalous entries in the tensor.

<!--chapter:end:00-abstract.Rmd-->

#Introduction

##Anomaly Detection

  Anomaly detection is used to identify unusual patterns or observations that do not conform to expected behavior in a dataset. Anomalies can be broadly categorized into three categories:

  Point anomalies: A single instance of data is anomalous if it's too far off from the rest. For example detecting credit card fraud based on a single spending spree that represents the credit card being stolen and used.

  Contextual anomalies: The abnormality is context specific. This type of anomaly is common in time-series data. For instance, high spending on food and gifts every day during the holiday season is normal, but may be considered unusual otherwise.

  Collective anomalies: A set of data observations that when collectively assessed helps in detecting anomalies. For instance, repeated pings from a certain IP address to a port connection on a hosted network may be classified as a port scanner, which often preludes a network attack. 

##Network Attacks 

  Network security is becoming increasingly relevant as the flow of data, bandwith of transactions, and user dependency on hosted networks increase. As entire networks grow in nodes and complexity, attackers gain easier entry points of access to the network. The most benign of attackers attempt to shutdown networks (e.g. causing a website to shutdown with repeated pings to its server), while more malicious attempts involve hijacking the server to publish the attacker's own content or stealing unsecured data from the server, thus compromising the privacy of the network's users.

  Attackers follow a specific three step strategy when gathering intelligence on a network, the most important component of which is scanning. Network scanning is a procedure for identifying active hosts on a network, the attacker uses it to find information about the specific IP addresses that can be accessed over the Internet, their target's operating systems, system architecture, and the services running on each node/computer in the network. Scanning procedures, such as ping sweeps and port scans, return information about which IP addresses map to live hosts that are active on the Internet and what services they offer. Another scanning method, inverse mapping, returns information about what IP addresses do not map to live hosts; this enables an attacker to make assumptions about viable addresses.

  All three of these scanning methods leave digital signatures in the networks they evaluate because they apply specific pings that are then stored in the network logs. Most scanners use a specific combination of bytes, packets, flags (in TCP protocol), and ports in a sequence of pings to a network. Identifying a scanner's often many IP addresses from the set of pings available in the network's logs is thus an anomaly detection problem. In particular, because the data is unlabeled, meaning it is unclear which observations are actually scanners and which are just standard user behavior, unsupervised approaches are necessary for tackling the problem.
  
  This particular dataset is from Duke University's Office of Information Technology (OIT), and it covers all observations in their network traffic during a five minute period in February 2017.

###Status Quo Solution

  OIT's current solution for detecting scanners relies on specific domain knowledge gathered from diagnostics programs and data analysis completed on previous data. They prevent scanners by blocking IP addresses that fit certain rules they have constructed to run on every network transaction as it occurs. The specific checks in these rules are private for security reasons, but they belong to the nature of evaluating the size of transactions, repeated connections between particular ports, many pings from the same address, and combinations of these particular behaviors.

  While this solution presents a methodical way for banning IP addresses and its method of rule checking is essentially removing what OIT considers outliers for network transactions-any observation that does not fit within the constraints specified by the rules is classified as an outleir and its source IP is blocked-it is inflexible, prone to detecting false negatives, and fails to detect  observations that may be within the parameter constraints of the rules but are anomalous with respect to other parameters or parameter constraints.
  
## Network Dataset

### Features

  The networks dataset contains 13 features, 8 categorical and 5 continuous, and the observations are unlabeled (not specified whether they are considered a scanner). The 13 features are: 

**Continuous:**

- StartTime (Start Time): the time when the observation is logged
- SrcBytes (Source Bytes): the total number of bytes sent in the observation 
- SrcPkts (Source Packets): the number of packets sent in the observation
- DstBytes (Destination Bytes): the total number of bytes received in the observation
- DstPkts (Destination Packets): the number of packets received in the observation
Note, the destination packets and bytes features do not have the same values as their source counterparts because the connections are compressed and decompressed into different forms and byte sizes when sent. For instance, it is possible for the number of destination packets to be larger than source packets. It is also possible for information to be lost during the connection.

**Categorical:**

- Flgs (connection flag): flow state flags seen in transaction between the two addresses
- Proto (network protocol): specifies the rules used for information exchange via network addresses. Transmission Control Protocol (TCP) uses a set of rules to exchange messages with other Internet points at the information packet level, and Internet Protocol (IP) uses a set of rules to send and receive messages at the Internet address level.
- SrcAddr (Source Address): the IP address of the connection's source 
- DstAddr (Destination Address): the IP address of the connection's destination
- Sport (Source Port): the network port number of the connection's source. A port numbers identifies the specific process to which a network message is forwarded when it arrives at a server. 
- Dport (Destination Port): the network port number of the connection's destination 
- Dir (direction): the direction of the connection 
- State (connection state): a categorical assessment of the current phase in the transaction when the timestamp is recorded

Note, the addresses have been anonymized for security reasons.

### Argus

  Argus is the open source network security tool applied to network transactions that collects the data for the features. The Argus wiki and the OIT manual provides key insights into the structure and nature of the data. Specifically, the sessions are clustered together by address, so the pytes and packets values are accumulative over a set duration and each session has its own start time but does not have a tracked end time. There exist 2-4 million connections on average every 5 minutes. Furthermore the protocol in this dataset is always gathered from TCP protocol and the direction will always be to the right (i.e. Source to Destination). This information supports dropping proto, StartTime, and Direction from the dataset for future analysis because they do not present any information regarding whether an observation can be considered an anomaly. Furthermore, the State feature may not be reliable because Argus occasionally resets the state data statistics during monitoring.


<!--chapter:end:01-intro.Rmd-->

# Modeling Port Relationships 

```{r, include=FALSE}
library(lattice)
library(reshape)
library(igraph)
setwd("~/Desktop/Stats Thesis/thesis-sp18-wu-anomalydet/index")
argus = readRDS("data/argus_complete.rds")
means = readRDS("data/means.RDS")
vars = readRDS("data/vars.RDS")
freqs = readRDS("data/freqs.RDS")
combos = readRDS("data/combos.RDS")

n_Sport = 20
n_Dport = 20

#get freqs
Sport_table = as.data.frame(table(argus$Sport))
Sport_table = Sport_table[order(-Sport_table$Freq),]
top_Sport = (head(Sport_table$Var1, n_Sport))

#get freqs
Dport_table = as.data.frame(table(argus$Dport))
Dport_table = Dport_table[order(-Dport_table$Freq),]
top_Dport = (head(Dport_table$Var1, n_Dport))
```

## Motivation

  Preliminary data analysis signaled that there may exist trends between different port combinations. For instance, a particular source and destination port may frequently contain large byte transactions in their connections. Devising a systematic way to identify these combinations may present outliers that can be further investigated for scanner behavior. 

  This approach to the anomaly detection problem reduces the dataset to the values of the four continuous features, SrcBytes, SrcPkts, DstBytes, DstPkts, observed across different source port and destination port combinations. The data can be represented as a 3-dimensional tensor $Y \in \mathbb{R_{m \times n \times 4}}$ where $m$ represents the number of source ports, $n$ represents the number of destination ports, and $4$ accounts for the four continuous features in the dataset. Each cell, $y_{ijk}$, contains the mean of all the observations observed for source port, $i$, and destination port $j$. In the cases where the combination of $i$ and $j$ is not observed in the dataset, $y_{ijk}$ is missing.

  The goal of this paper is to devise an optimal strategy for imputing the missing cells in $Y$ to create the completed tensor $Y' \in \mathbb{R_{m \times n \times 4}}$. As new observations are observed for combinations of ports $i$ and $j$, the $y'_{ijk}$ values can be interpreted as an approximation for the expected behavior for that particular port combination. Observations with continuous features that are a certain threshold away from $y'_{ijk}$ may be marked as anomalies and investigated further.

  Imputing values for each of the four continuous features in the dataset for all possible source and destination port combinations yields a reasonable expected value in each cell of the ports matrix that can then be compared to actual connection values when they are observed. New observations that differ greatly from the imputed values are flagged as anomalies and require further investigation. 

## Tensor Properties

The following properties of $Y$ inform the imputation strategies in following sections.

### Correlations

```{r, echo = FALSE}
### CORRELATIONS
kendall_cors = matrix(c(1.0000000, 0.7227563, 0.5729918, 0.6367907,
                0.7227563, 1.0000000, 0.7425211, 0.8108184,
                0.5729918, 0.7425211, 1.0000000, 0.8827988,
                0.6367907, 0.8108184, 0.8827988, 1.0000000),
                nrow = 4, ncol = 4)
rownames(kendall_cors) = c("SrcBytes", "SrcPkts", "DstBytes", "DstPkts")
colnames(kendall_cors) = c("SrcBytes", "SrcPkts", "DstBytes", "DstPkts")
levelplot(kendall_cors, main = "Kendall Correlations Between Continuous 
          Features", xlab = "", ylab = "")
#continuous_data = subset(argus, select = c("SrcBytes", "SrcPkts", "DstBytes", "DstPkts"))
#sub_data = continuous_data[1:50000,]
#cor(continuous_data, method = "kendall")

#           SrcBytes   SrcPkts  DstBytes   DstPkts
# SrcBytes 1.0000000 0.7227563 0.5729918 0.6367907
# SrcPkts  0.7227563 1.0000000 0.7425211 0.8108184
# DstBytes 0.5729918 0.7425211 1.0000000 0.8827988
# DstPkts  0.6367907 0.8108184 0.8827988 1.0000000

```

The matrix above describes the Kendall rank correlations (commonly referred to as Kendall's tau coefficent) between the four continuous features in the dataset.
<!-- A tau test is a non-parametric hypothesis test for statistical dependence based on the tau coefficient. -->

Intuitively, the Kendall correlation between two features will be high when observations have a similar rank (i.e. relative position label of observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar rank between the two variables. The range of correlations is [-1, 1]. Kendall correlation was selected as a measure because it evaluates ranks between observations, as opposed to Pearson, which is more susceptible to outliers in the dataset (large byte and packet observations in the continuous feautres skewed the Pearson measures). 

### Missingness

```{r, echo = FALSE}
### MISSINGNESS VISUALIZATIONS

#sum(apply(means, MARGIN = 1, FUN = function(x) length(x[is.na(x)])))

missing = matrix(list(), nrow = n_Sport, ncol = n_Dport)
dimnames(missing) = list(top_Sport, top_Dport)
for (s in 1:n_Sport){
  for (d in 1:n_Dport){
    if(freqs[s,d] == 0){
      missing[s,d] = 1
    }
    else{
      missing[s,d] = 0
    }
  }
}
levelplot(missing,xlab = "Source Ports", ylab = "Destination Ports", 
          main = "Missingness in Ports Matrix", col.regions=gray(0:1,1),cuts=1)
```

The above matrix represents the missingness in the port combinations for pairings of the top 20 most used source ports and destination ports. The black cells represent missingness; of the 400 cells in the matrix, 295 (73.75%) of cells are missing observations.

### Row and Column Properties

```{r, echo = FALSE}
### ROW MEANS/COL MEANS VISUALIZATION
row_means = rowMeans(means, na.rm = TRUE)
col_means = colMeans(means, na.rm = TRUE)

par(mfrow=c(2,1))
barplot(row_means, las=2, xlab = "Source Port", ylab = "Mean (SrcBytes)",
        main = "Row Means of SrcBytes Matrix")
barplot(col_means, las=2, xlab = "Destination Port", ylab = "Mean (SrcBytes)",
        main = "Column Means of SrcBytes Matrix")
```

The bar plots above represent the row and column means of the continuous features for each slice of the tensor. These row means and column means inform imputation techniques for the missing cells within those respective rows and columns. COMMENT ON DISPROPORTIONATE MEANS BASED ON MISSING VARIANCES

### Port Connections

```{r, echo=FALSE}
### PORTS NETWORK GRAPH
# net = graph_from_adjacency_matrix(missing)
# net <- simplify(net, remove.multiple = T, remove.loops = T) 
# 
# 
# # Compute node degrees (#links) and use that to set node size:
# deg <- degree(net, mode="all")
# V(net)$size <- deg*3
# # We could also use the audience size value:
# V(net)$size <- V(net)$audience.size*0.6
# 
# E(net)$width <- E(net)$weight/6
# #change arrow size and edge color:
# E(net)$arrow.size <- .2
# E(net)$edge.color <- "gray80"
# plot(net, edge.arrow.size=.2, edge.color="orange",
#      vertex.color="orange", vertex.frame.color="#ffffff",
#      vertex.label.color="black") 


# Dataframe way
test = melt(means)
test = na.omit(test)
g = graph.data.frame(test)

V(g)$type <- 1
V(g)[name %in% top_Dport]$type <- 2
V(g)[name %in% top_Sport]$type <- 3
deg <- degree(g, mode="all")
V(g)$size <- deg
E(g)$width <- E(g)$weight * 3
E(g)$arrow.size <- .2
g = simplify(g, remove.multiple = T, remove.loops = T) 
shape <- c("circle", "square", "circle")
col <- c("orange", "steelblue","red")
plot(g,
     vertex.color = col[V(g)$type],
     vertex.shape = shape[V(g)$type],
     edge.arrow.size = 0.2
)
```

Describe Ports network graph

### Matrix Slice Properties

```{r, echo = FALSE}
### MATRIX VISUALIZATIONS
# https://stackoverflow.com/questions/5453336/plot-correlation-matrix-into-a-graph
# levelplot(means, xlab = "Source Ports", ylab = "Destination Ports", 
#           main = "Means of SrcBytes By Ports", 
#           col.regions=rgb.palette(120))
freqs20 = readRDS("data/freqs_20.RDS")
levelplot(freqs20, xlab = "Source Ports", ylab = "Destination Ports",  
          col.regions = heat.colors(16)[length(heat.colors(16)):1],
          main = "Sample Sizes of Port Combinations")
```

Variance in sample size 0 to 35000

<!--chapter:end:02-eda.Rmd-->

# Alternating Least Squares Applied to Two Dimensional Matrices

## Motivation 

The preliminary tensor imputation technique slices the tensor into four separate matrices, $Y^{(1)}, Y^{(2)}, Y^{(3)}, Y^{(4)} \in \mathbb{R}_{m \times n}$. Each matrix has dimensions defined by the number of source ports by the number of destination ports. Each cell, $Y^{(k)}_{ij}$ contains the mean of the observations observed for continuous feature $k$ and source port $i$ destination port $j$. Each matrix will have its missing cells imputed with information from the rest of that matrix, and separately from information about the other matrices. The four completed matrices will then be recombined to create the tensor $Y$. Similar techniques for matrix completion were employed in the Netflix Challenge where top competitor predicted ratings for movies by users that had not watched the movie based on the other ratings in the matrix of users and movies.

## Matrix Completion Algorithm

Each $Y^{(k)}$ has missingness because not every source port interacts with every destination port. $F \in \mathbb{R}_{m \times n}$ is a sparse matrix that represents the frequencies of combinations, i.e $F[32242,12312]$ represents the number of observations for the 32242 12312 port interaction. $M \in {\rm I\!R}^{m \times n}$ represents a boolean matrix of whether the corresponding $Y$ values are missing. $Y^{(k)}[M]$ represents all of the missing values of $Y^{(k)}$. Moreover because each non-missing observation contains all four continuous features, $Y[M]$ represents all the missing values of $Y$.

The objective is $$min \sum_{i,j:F_{i,j} > 0} (Y^{(k)}_{i,j} - u^{(k)}_iD^{(k)}v^{(k)T}_j)^2$$ where $U^{(k)}D^{(k)}V^{(k)T}$ represents the singular value decomposition of $Y^{(k)}$. There are multiple steps to the matrix completion process:

### ANOVA Initial Imputation

Impute the initial values for the missing $y^{(k)}_{i,j}$ observations $1 \leq i \leq m, 1 \leq j \leq n$: In general an additive model is applicable: $$y^{(k)}_{i,j} = \mu^{(k)} + a^{(k)}_i + b^{(k)}_j + \epsilon^{(k)}_{i,j}$$ where $\epsilon^{(k)} \in N(0,\sigma^2)$, $\mu^{(k)}$ is the overall mean of $Y^{(k)}$, $a^{(k)}_i$ is the row mean, and $b^{(k)}_j$ is column mean. An analysis of variance (ANOVA) imputation is used to fill in the initial values, $y^{(k)}_{i,j}$. Ignoring the missing values for now, let $y_{..}$ denote the empirical overall mean, $y_{i.}$ denote the empirical row mean, and $y_{.j}$ denote the column mean. $$y_{i,j} = y_{..} + (y_{i.}-y{..}) + (y_{.j}-y_{..}) = y_{i.} + y_{.j} - y{..}$$

### Repeated Imputation

The repeated imputation procedure solves $Y^{(s)}[M] = R_k(Y^{(s-1)})[M]$ where $R_k$ is the best rank-k approximation for the $s$-th step. For each step $(s)$ use singular value decomposition to decompose $$Y^{(s)} =  U^{(s)}DV^{T(s)}$$ where $D$ is a diagonal matrix of the singular values, $U$ is the left singular vectors of $Y$ and $V$ is the right singular vectors of $Y$. 

The Eckart-Young-Mirsky (EYM) Theorem provides the best rank-k approximation for the missing values in $Y^{(s+1)}$. Recall $Y[M]$ represents all of the missing values of $Y$. Applying the EYM theorem:  $$Y^{(s+1)}[M] = (U[,1:k]^{(s)}D[,1:k]V[,1:k]^{T(s)})[M]$$ Where $U[,1:k]$ represents the first $k$ columns of $U$ and the same for $D$ and $V$. 

### Convergence Criterion

The EYM rank approximation imputation steps are repeated until the relative difference between $Y^{(s+1)}$ and $Y^{(s)}$ falls below a set threshold, $T$. The relative difference threshold is expressed: $$\frac{\|Y^{(s+1)}-Y^{(s)}\|_2}{\|Y^{(s)}\|_2} < T$$ where $\|Y\|_2$ is the Frobenius norm. The denominator of the expression ensures the convergence criterion is invariate to a scale change in the matrix itself. 

### Implementation

```{r,include=FALSE}
Y = readRDS("data/means_SB.rds")
M = readRDS("data/freqs.rds")
# ## REMOVING FULL MISSING COLUMNS FROM MATRICES
# Y = Y[, colSums(is.na(Y))!=nrow(Y)] #remove NA cols
# Y = Y[rowSums(is.na(Y)) != ncol(Y),] #remove NA rows
# ## REMOVING FULL 0 COLUMNS FROM MATRICES
# M = M[ , !apply(M==0,2,all)]
# M = M[ !apply(M==0,1,all) , ]
####Eckhart Young Theorem Implementation, Best Rank k Approximation####
matrix_complete = function(S = 1000, k = 2, nrows, ncols, Y, M){
  Y_imputed = Y
  #overall mean
  n = sum(M)
  mu = sum(Y, na.rm = TRUE)/n
  #calculate row means and col means
  a_i = rowMeans(Y, na.rm = TRUE)
  b_j = colMeans(Y, na.rm = TRUE)
  #set NaN to 0 in means to fix anova fill in
  a_i = sapply(a_i, function(x) if (!is.finite(x)) {0} else {x})
  b_j = sapply(b_j, function(x) if (!is.finite(x)) {0} else {x})
  #Fill in missing values in Y_imputed with ANOVA
  #Y_imputed = outer(1:nrow(Y), 1:ncol(Y), function(r,c) ifelse(M[r,c] == 0, a_i[r] + b_j[c] - mu, Y[r,c]))
  for (i in 1:nrows){
    for (j in 1:ncols){
      if (M[i,j] == 0){
        Y_imputed[i,j] = a_i[i] + b_j[j] - mu
      }
    }
  }
  for (s in 1:S){
    #extract SVD
    svd_Y = svd(Y_imputed)
    U = svd_Y$u
    V = svd_Y$v
    #EYM theorem
    if (k == 1){
      EYM = (matrix(U[,1:k]) * (svd_Y$d)[1:k]) %*% t(matrix(V[,1:k]))
    }
    else {
      EYM = U[,1:k] %*% diag((svd_Y$d)[1:k]) %*% t(V[,1:k])
    }
    for (i in 1:nrows){
      for (j in 1:ncols){
        if (M[i,j] == 0){
          Y_imputed[i,j] = EYM[i,j]
        }
      }
    }
  }
  return (Y_imputed)
}
```

### Leave One Out Cross Validation

To assess the quality of the imputation, Leave-One-Out Cross Validation (LOOCV) is used to generate a prediction error. LOOCV cycles through the observed values, setting each to NA (missing), and then performing the described imputation process. The prediction error is then calculated as some function of the difference between the imputed value and the true value. In this case, the algorithm records absolute error $\sum \mid \hat y_{i,j} - y_{i,j}\mid$ and root mean square error $\sqrt{\frac{\sum (\hat y_{i,j} - y_{i,j})^2}{n}}$ where $n$ is the number of observations not missing.

```{r,include=FALSE}
#Leave One Out Cross Validation
loocv = function (S = 1000, k = 2, nrows = nrows, ncols = ncols, Y, M){
  error = 0
  rmse = 0
  n = 0
  for (i in 1:nrows){
    for (j in 1:ncols){
      if (M[i,j] != 0){
        n = n + 1
        M_imputed = M
        true_sd = Y[i,j]
        M_imputed[i,j] = 0
        Y_imputed = matrix_complete(S, k, nrows, ncols, Y, M_imputed)
        error = error + abs((Y_imputed[i,j] - Y[i,j]))
        rmse = rmse + (Y_imputed[i,j] - Y[i,j])^2
      }
    }
  }
  rmse = sqrt(rmse/n)
  return (list(Error = error, RMSE = rmse, Observations = n))
}
```

## Validation Against Simulated Data

Before applying the algorithm on the real data it is useful to validate the algorithmic approach against simulated data where the true rank is known. 

### Simulating a Low Rank Matrix 

Taking the Kronecker product of two lower dimension matrices yields a higher dimension matrix with low rank. Explicitly, given matrix $A \in \mathbb{R}_{m \times r}$ and $B \in \mathbb{R}_{r \times n}$, $A \otimes B = C$ where $C \in \mathbb{R}_{m \times n}$ with rank $r$. Thus, when $r < m, r < n$ the matrix $C$ has an optimal low rank that minimizes the root mean square error from the leave one out cross validation procedure. To add noise to the simulated matrix, $C$, simply add an error matrix, $E \in \mathbb{R}$ where $e_{i,j} \sim N(0,1)$.

Moreover, this procedure provides a computationally efficient way to simulate many random low rank matrices to use as inputs for the validation procedure. In the case of simulated matrices, there is no missing entries, so the leave one out cross validation procedure sequentially removes each cell in the matrix, imputes its value using the rank being investigated, and considers the error as a function of the difference between the true value and the imputed value. 

WHAT PLOTS OR VISUALIZATIONS DO I USE FOR RESULTS? HISTOGRAM OF PERCENTAGES FOR RANK APPROXIMATIONS? 1-4 100%, higher ranks arent that relevant; effects of noise

USE THIS TECHNIQUE TO SHOW DIFFERENT EFFECTS OF NOISE? 

```{r,include=FALSE}
# generate m x n matrix with rank r, add noise
generate_low_rank_matrix = function(m, n, r, noise = FALSE){
  A = matrix(rnorm(m * r, mean=0, sd=1), m, r) 
  B = matrix(rnorm(r * n, mean=0, sd=1), r, n)
  AB = A %*% B
  if (noise){
    E = matrix(rnorm(m * n, mean=0, sd=1), m, n)
    AB = AB + E
  }
  return (AB)
}
```


### Approximating Optimal Rank

```{r, include=FALSE}

# sets each elements to missing one at a time
full_loocv = function (S = 1000, k, nrows, ncols, Y){
  error = 0
  rmse = 0
  n = 0
  for (i in 1:nrows){
    for (j in 1:ncols){
      n = n + 1
      M = Y
      M[i,j] = 0
      Y_imputed = matrix_complete(S, k, nrows, ncols, Y, M)
      error = error + abs((Y_imputed[i,j] - Y[i,j]))
      rmse = rmse + (Y_imputed[i,j] - Y[i,j])^2
    }
  }
  rmse = sqrt(rmse/n)
  return (list(Error = error, RMSE = rmse, Observations = n))
}

approximate_rank = function(Y, M, S = 50, simulated = TRUE){
  nrows = nrow(Y)
  ncols = ncol(Y)
  ranks = c()
  all_errors = list()
  for (i in 1:S){
    cat("Approximation Round: ", i, '\n')
    if (simulated){
      cv_errors = lapply(seq(1,10,1), function(k) full_loocv(200,k,nrows,ncols,Y)$RMSE)
    }
    else{
      cv_errors = lapply(seq(1,10,1), function(k) loocv(200,k,nrows,ncols,Y, M)$RMSE)
    }
    low_rank = which.min(cv_errors)
    ranks = c(ranks, low_rank)
  }
  return (ranks)
}
```

## Results on Real Data

```{r,eval = FALSE,echo=FALSE}
# RMSEs = lapply(seq(1,8,1), function(k) loocv(250,k, nrow(Y), ncol(Y), Y, M)$RMSE)
# [[1]]
# [1] 42727.19
# 
# [[2]]
# [1] 31878.69
# 
# [[3]]
# [1] 44605.58
# 
# [[4]]
# [1] 48193.36
# 
# [[5]]
# [1] 46903.49
# 
# [[6]]
# [1] 47509.22
# 
# [[7]]
# [1] 47807.57
# 
# [[8]]
# [1] 48136.36
rmses = c( 42727.19, 331878.69, 44605.58, 48193.36, 46903.49, 47509.22, 47807.57, 48136.36 )
plot (seq(1,8,1), rmses)
```

The above plot displays the root mean square errors from leave one out cross validation across different rank inputs into the algorithm. It's clear that rank 2 provides the best low-rank solution for the alternating least squares imputation algorithm. Thus rank 2 is used in the algorithm to impute the missing values in $Y$. 

```{r, echo = FALSE}
### version of EYM that replaces the considers the entire fitted matrix
matrix_complete = function(S = 1000, k = 2, nrows, ncols, Y, M){
  Y_imputed = Y
  #overall mean
  n = sum(M)
  mu = sum(Y, na.rm = TRUE)/n
  #calculate row means and col means
  a_i = rowMeans(Y, na.rm = TRUE)
  b_j = colMeans(Y, na.rm = TRUE)
  #set NaN to 0 in means to fix anova fill in
  a_i = sapply(a_i, function(x) if (!is.finite(x)) {0} else {x})
  b_j = sapply(b_j, function(x) if (!is.finite(x)) {0} else {x})
  #Fill in missing values in Y_imputed with ANOVA
  #Y_imputed = outer(1:nrow(Y), 1:ncol(Y), function(r,c) ifelse(M[r,c] == 0, a_i[r] + b_j[c] - mu, Y[r,c]))
  for (i in 1:nrows){
    for (j in 1:ncols){
      if (M[i,j] == 0){
        Y_imputed[i,j] = a_i[i] + b_j[j] - mu
      }
    }
  }
  for (s in 1:S){
    #extract SVD
    svd_Y = svd(Y_imputed)
    U = svd_Y$u
    V = svd_Y$v
    #EYM theorem
    if (k == 1){
      EYM = (matrix(U[,1:k]) * (svd_Y$d)[1:k]) %*% t(matrix(V[,1:k]))
    }
    else {
      EYM = U[,1:k] %*% diag((svd_Y$d)[1:k]) %*% t(V[,1:k])
    }
    Y_imputed = EYM
  }
  return (Y_imputed)
}
Y_imputed = matrix_complete(250, 2, nrow(Y), ncol(Y), Y, M)
par(mfrow = c(1,2))
plot(Y[!is.na(Y)], Y_imputed[!is.na(Y)])
abline(0,1, col = "red")
plot(Y[!is.na(Y)], Y_imputed[!is.na(Y)], xlim = c(0,1000), ylim = c(0,1000))
abline(0,1, col = "red")
```

The two plots above display the true values of the $Y$ matrix (i.e. the non-missing values) versus their corresponding fitted values using the alternating least squares algorithm with an input of rank 2. The first plot displays all values and shows a somewhat positive linear trend (an ideal fit of the true values would be a scatter of points following the 45 degree angled line represented in red). However, several outliers with large true values skew this dataset and cause the plot to appear linear. Closer examination of the true and fitted values smaller than 1000 (the plot on the right) reveals the relationship is far from the linear pattern.

### Scale Transformations

The poorly fitted results motivates a consideration of the scale of the data. The present algorithm uses the sample averages of the overall matrix as well as the row and column means when imputing each missing cell value. This reliance upon sample means leads to susceptiblity to outliers. Moreover exploratory data analysis reveals the dataset contains outliers, particularly in the SrcBytes and DstBytes measurements. Thus, a transformation of features in the model may be appropriate for improving the fit of the algorithm.

When a natural log transformation is applied to the raw dataset before any imputation steps are taken, the alternating least squares imputation algorithm yields the following root mean square errors varied by rank. Note, these errors have been reverse transformed with exponentiation to remain on the same scale as the errors from the non-transformed dataset.











<!-- While matrix completion via singular value decomposition presents valid missing value imputations, and the algorithm converges relatively quickly, the error generated from leave one out cross validation reflects that the imputation performs rather poorly for low rank solutions to the data. Moreover, the errors are minimized at a rank approximation of 3, but even at this rank, the errors are relatively high considering the data was first normal transformed.  -->

<!-- This poor performance may largely be due to the fact the algorithm does not account for the variability in the number of observed solutions for each cell being imputed. Unlike the Netflix Competition, in which each cell of the matrix being completed contained only a single user rating of a movie, the matrix in this problem contains the average of a variable number of observations in each cell. -->


<!--chapter:end:03-als.Rmd-->

# A Bayesian Approach to Two Dimensional Matrix Completion

## Motivation

The previous section's results reflected the need for an imputation strategy that accounted for the variability in the number of observations observed for each port combination when imputing that particular combination's cell. The previous technique fails to take into account the differing sample size and variance in each cell (source port $i$, destination port $j$), so the algorithm treated each $y_{ij}^{k}$, which represents the mean of all observations in cell $(i,j)$ for continuous feature $k$, as a single value with no consideration of differing sample sizes and variances between cells. The following section constructs a statistical model that takes frequency of observations for each cell into account and repeatedly samples from that statistical model to complete the matrix.

## Statistical Model for Port Relationships

Additive Main Effects and Multiplicative Interaction Models (AMMI models) provide a defined statistical model for each cell in the ports matrix. In particular, the model combines the additive effects of the initial ANOVA imputation with the multiplicative effects yielded from singular value decomposition described in the previous section. More importantly, the model also includes a variance term for each cell that takes into account the differing frequency of observations in each port combination. 

The following statistical model is defined for imputing port relationships: $$y_{i,j} = u_i^Tv_j + \frac{\sigma_i \tau_j}{\sqrt{n_{ij}}}\epsilon_{ij}$$ where $u_i$ represents ______, $v_j$ represents _____, $\sigma_i$ represents the MEAN??? of the standard deviations of each row in the matrix, $\tau_j$ represents the MEAN??? of the standard deviations of each column in the matrix, and $\epsilon_{ij} \sim N(0,1)$. Fixing specific rows or columns in the analysis enables the model to be rewritten in the form of a weighted least squares. In particular, fixing the $j$th row of the matrix yields: $$y_i = \beta^Tx_i + \sigma w_i\epsilon_{ij}$$ where $w_i = (\frac{\tau_j}{\sqrt(n_{ij}})^2$, and $\beta^T = u_i$. Note, this technique again slices the tensor into the four separate matrices, $Y^{(1)}, Y^{(2)}, Y^{(3)}, Y^{(4)} \in \mathbb{R}_{m \times n}$, and the model can be applied to each matrix $Y^{(k)}$ separately.

### Generalized Least Squares Model

Vectorizing the above model yields $$\tilde{y} = X\beta + \sigma W^{1/2} \epsilon$$ where $W \in \mathbb{R}_{m \times n}$ is the diagonal matrix of weights, such that $$
  W =
  \begin{bmatrix}
    w^_{1} & & \\
    & \ddots & \\
    & & w^_{m}
  \end{bmatrix} 
  = \begin{bmatrix}
    \frac{\tau_1^2}{n_{11}} & & \\
    & \ddots & \\
    & & \frac{\tau_n^2}{n_{mn}}
  \end{bmatrix}$$

Thus in matrix form the full model is expressed: 

With no errors or missing values this matrix of $v_j^T$ is the right singular matrix $V$ of the singular value decomposition of $Y$.


This is a specific form of the Generalized Least Squares Model (GLS), which gives a weighted least squares estimate of $\beta$, and it is appropriate when the error terms are not independent and identically distributed. Bayesian analysis of this problem provides similar parameter estimates to GLS, and both ordinary least squares and GLS provide unbiased parameter estimates of $\beta$ with the latter giving estimates with a lower variance. 

The full conditional distributions of the random variables $\beta$ and $\sigma^2$ for this case of GLS are descrbied below: 
$$\{\beta \mid X, \vec{y}, \sigma^2\} \sim MVN (\beta_n, \Sigma_n)$$
$$\{\sigma^2 \mid X, \vec{y}, \beta\} \sim IG (\frac{\nu_0 + n}{2}, \frac{v_0\sigma^2_0 + SSR_W}{2})$$
where MVN represents the Multivariate Normal Distribution, IG represents the Inverse Gamma distribution, W is described above, and 
$$ \begin{cases}
      \Sigma_n = (X^TWX\sigma^2+W_0^{-1})^{-1}\\
      \beta_n = \Sigma_n(X^TWy\sigma^2 + W_0^{-1} \beta_0)
    \end{cases}$$
$$SSR_W = (y - X\beta)^TW(y-X\beta)$$

The remaining variables in the closed form full conditionals come from the random variables prior distributions, which are defined as follows: 
$$\beta \sim MVN (\beta_0, W_0)$$
$$\sigma^2 \sim IG (\frac{\nu_0}{2}, \frac{v_0}{2}\sigma_0^2)$$

The initial values for the prior distributions are set as: $\beta_0 = 0$, $W_0 = \gamma^2I$ where $\gamma^2$ is a large number and $I$ is the $m \times n$ identity matrix, $\nu_0 = 2$, $\sigma_0^2 = 1$. This results in a diffuse prior for $\beta$ that spreads out the density, and a noninformative prior for $\sigma^2_0$. 


## Gibbs Sampler

Following the formulation of the model, a Gibbs Sampling is created to repeatedly generate samples from the full conditional of each parameter in the statistical model, which iteratively creates an approximate value for each cell. 

The Gibbs Sampler algorithm progresses as follows: 

Let the set of parameter samples at step $s$ in the algorithm be defined as $\phi^{(s)} = \{\beta^{(s)}, \sigma^{2(k)}\}$. 

1. Sample $\beta^{(s+1)} \sim P(\beta \mid X, \vec{y}, \sigma^{2(k)})$

2. Sample $\sigma^2 \sim P(\sigma^2 \mid X, \vec{y}, \beta^{(s+1)})$

Set $\phi^{(s+1)} = \{\beta^{(s+1)}, \sigma^{2(k+1)}\}$


## Full Gibbs Sampler for Imputing Missing Values

The Gibbs Sampler described represents the steps to sampling for $u_i$

Initialize $\sigma_i$ and $\tau_j$ as the overall standard deviation of the $Y$ matrix. 



<!--chapter:end:04-gibbs.Rmd-->

# Tensor Completion

## Imputation Strategy

The imputation strategy focuses on finding a low rank approximation for $Y$ when decomposing the tensor.

### CP Decomposition

The CP decomposition expresses the tensor as: $$Y = \sum_{r=1}^Ru_r \cdotp v_r \cdotp w_r$$ where $r$ represents the rank approximation, $\cdotp$ denotes the outer product of tensors, and $u \in \mathbb{R_{m \times r}}$, $v \in \mathbb{R_{n \times r}}$, and $w \in \mathbb{R_{4 \times r}}$. Each individual cell is expressed: $$y_{ijk} = \sum_{r=1}^Ru_{ri} \cdotp v_{ri} \cdotp w_{ri}$$ Applying this decomposition yields the objective $$min_{Y'}\|Y-Y'\|, Y' = \sum_{r=1}^R\lambda_r(u_r \cdotp v_r \cdotp w_r)$$, where $lambda_r$ is the regularization penalty.

### Variable Sample Sizes

A traditional approach to tensor completion involves using alternating least squares regression to impute the missing values after populating them with some initial values. The previous section applies this approach to a 2-dimensional $m \times n$ tensor that represents a cross-section slice of $Y$ that only includes one of the four continuous features. 
*include als section here, related work: https://arxiv.org/abs/1410.2596 (hastie fast als), application netflix challenge*

While this approach yields a completed tensor, it does not account for the fact that the means in each cell are calculated from a variable number of observations. Furthermore it is not necessarily true that $n_{ijk} = n_{i'j'k'}$ or $\sigma^2_{ijk} = \sigma^2_{i'j'k'}$ for $i \neq i', j \neq j', k \neq k'$. 

We propose the following model: 
$$y_{ijk} \sim N(\mu_{ijk}, \frac{\sigma^2_{ijk}}{n_{ijk}})$$ 
where $\mu_{ijk}$ is the sample mean, $n_{ijk}$ is the sample size, and $\sigma^2_{ijk}$ is the sample variance of observations for source port $i$, destination port $j$, and continuous feature $k$.

Substituting these values into the Gaussian probability density function yields the likelihood: $$\frac{n_{ijk}}{\sigma^2_{ijk}}\sum(\bar y_{ijk} - \mu_{ijk})^2$$

Applying the CP/PARAFAC decomposition $u_{ijk}$ is re-expressed: $$u_{ijk} = \sum_{r=1}^Ra_{ir}b_{jr}c_{kr}$$

Vectorizing the inputs in the likelihood yields: $$\sum_j\sum_k[\bar y_{ijk} - a_i^T(b_i \cdotp c_k)]\frac{n_{ijk}}{\sigma^2_{ijk}} (1)$$ where $a_i \in \mathbb{R_{m \times r}}$, $b_j \in \mathbb{R_{n \times r}}$, and $c_k \in \mathbb{R_{4 \times r}}$. Summing across $j$ and $k$ in this case solves for the $ith$ row slice of the tensor. How to notate vectorization of $y$?

Recall the Residual Sum of Squares (RSS) of the likelihood for an Ordinary Least Squares (OLS) regression is expressed: $$\sum_l(y_l-B^Tx_l)^2$$ 

Adding a weight, $w_l$ to the summation yields a Weighted Least Squares problem (WLS) $$\sum_l^nw_l(y_l-\beta^Tx_l)^2 (2)$$ that is analagous to the vectorized likelihood equation (1) with $w_l = \frac{n_{ijk}}{\sigma^2_{ijk}}$, $\beta = a_i$, $x = (b_j \cdotp c_k)$.

With this formulation its now possible to solve for the optimal values for each slice $a_i$ of the tensor.

Recall that in a traditional vectorized OLS, $y = X\beta + \sigma\epsilon$, where $y \in \mathbb{R_{n \times 1}}$, $X \in \mathbb{R_{n \times p}}$, $\beta \in \mathbb{R_{p \times 1}}$, and $\\sigma\epsilon \in \mathbb{R_{n \times 1}}$. Solving the maximum likelihood estimator of $\beta$, gives $\hat \beta = (X^TX)^{-1}X^Ty$. 

Applying this formulation to the weighted least squares gives $y = X\beta + W^{-\frac{1}{2}}\epsilon$. Solving for the weighted least squares estimator gives $\hat \beta = (X^TWX)^{-1}X^TWy$. 

Repeating this estimation technique across each slice of the tensor $a_i$, $b_j$, $c_k$ results in a completed model for $y_{ijk}$.

<!-- ## Motivation -->

<!-- The previous section's results reflected the need for an imputation strategy that accounted for the variability in the number of observations observed for each port combination when imputing that particular combination's cell. The following section constructs a statistical model that takes frequency of observations for each cell into account and repeatedly samples from that statistical model to complete the matrix.  -->

<!-- ## AMMI Model  -->

<!-- Additive Main Effects and Multiplicative Interaction Models (AMMI models) provide a defined statistical model for each cell in the ports matrix. In particular, the model combines the additive effects of the initial ANOVA imputation with the multiplicative effects yielded from singular value decomposition described in the previous section. More importantly, the model also includes a variance term for each cell that takes into account the differing frequency of observations in each port combination. Applying the same mathematical notation as the previous section, the model is formally expressed: $$y_{i,j} = u + a_i + b_j + \mathbf{u_i}D\mathbf{v_j^T} + \sigma_{i,j}\epsilon_{i,j}$$ -->
<!-- where $sigma_{i,j}$ is the variance for the $ith$ row $jth$ column in the ports combination matrix, and $\epsilon \sim N(0,1)$. -->

## Gibbs Sampling

Following the formulation of the model, a Gibbs Sampling algorithm is used to repeatedly generate samples from the full conditional of each parameter in the model statistical model, which iteratively creates an approximate value for each cell. 




<!--chapter:end:05-tensor.Rmd-->

# Conclusion {-}

If we don't want Conclusion to have a chapter number next to it, we can add the `{-}` attribute.

**More info**

And here's some other random info: the first paragraph after a chapter title or section head _shouldn't be_ indented, because indents are to tell the reader that you're starting a new paragraph. Since that's obvious after a chapter or section title, proper typesetting doesn't add an indent there.


<!--chapter:end:07-conclusion.Rmd-->

<!--
The bib chunk below must go last in this document according to how R Markdown renders.  More info is at http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html
-->

\backmatter

<!-- 
If you'd like to change the name of the bibliography to something else,
delete "References" and replace it.
-->

# References {-}
<!--
This manually sets the header for this unnumbered chapter.
-->
\markboth{References}{References}
<!--
To remove the indentation of the first entry.
-->
\noindent

<!--
To create a hanging indent and spacing between entries.  These three lines may need to be removed for styles that don't require the hanging indent.
-->

\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}


<!--
This is just for testing with more citations for the bibliography at the end.  Add other entries into the list here if you'd like them to appear in the bibliography even if they weren't explicitly cited in the document.
-->

---
nocite: | 
  @angel2000, @angel2001, @angel2002a
...

<!--chapter:end:99-references.Rmd-->

