---
title: "Low Rank Matrix Completion"
author: "James Wu"
date: "11/13/2017"
output:
  pdf_document: default
  html_document: default
---
```{r,echo=FALSE,include=FALSE,warning=FALSE}
library(softImpute)

setwd("~/Desktop/Stats Thesis/")
argus = readRDS("Dataset/argus_complete.rds")
```


#Low Rank Matrix Completion Via Singular Value Decomposition
There are $m$ source ports and $n$ destination ports. $Y \in {\rm I\!R}^{m \times n}$, is the matrix that stores the means of the combinations of source ports and destination ports. $Y$ has a lot of missingness because not every source port interacts with every destination port. $F \in {\rm I\!R}^{m \times n}$ is a sparse matrix that represents the frequencies of combinations, i.e $F[32242,12312]$ represents the number of observations for the 32242 12312 port interaction. $M \in {\rm I\!R}^{m \times n}$ represents a boolean matrix of whether the corresponding $Y$ values are missing. $Y[M]$ represents all of the missing values of $Y$.

There are multiple steps to the matrix completion process:
Impute the initial values for the missing $y_{i,j}$ observations $1 \leq i \leq m, 1 \leq j \leq n$: In general an additive model is applicable: $$y_{i,j} = \mu + a_i + b_j + \epsilon_{i,j}$$ where $\epsilon \in N(0,\sigma^2)$, $\mu$ is the overall mean, $a_i$ is the row mean, and $b_j$ is column mean. An analysis of variance (ANOVA) imputation is used to fill in the initial values, $y_{i,j}$. Ignoring the missing values for now, let $y_{..}$ denote the empirical overall mean, $y_{i.}$ denote the empirical row mean, and $y_{.j}$ denote the column mean. $$y_{i,j} = y_{..} + (y_{i.}-y{..}) + (y_{.j}-y_{..}) = y_{i.} + y_{.j} - y{..}$$

The repeated imputation procedure solves $Y^{(s)}[M] = R_k(Y^{(s-1)})[M]$ where $R_k$ is the best rank-k approximation for the $s$-th step. For each step $(s)$ use singular value decomposition to decompose $$Y^{(s)} =  U^{(s)}DV^{T(s)}$$ where $D$ is a diagonal matrix of the singular values, $U$ is the left singular vectors of $Y$ and $V$ is the right singular vectors of $Y$. 

The Eckart-Young-Mirsky (EYM) Theorem provides the best rank-k approximation for the missing values in $Y^{(s+1)}$. Recall $Y[M]$ represents all of the missing values of $Y$. Applying the EYM theorem:  $$Y^{(s+1)}[M] = (U[,1:k]^{(s)}D[,1:k]V[,1:k]^{T(s)})[M]$$. Eliminate (s), (s+1)

Where $U[,1:k]$ represents the first $k$ columns of $U$. 

The EYM rank approximation is repeated until the relative difference between $Y^{(s+1)}$ and $Y^{(s)}$ falls below a set threshold, $T$. The relative difference threshold is expressed: $$\|Y^{(s+1)}-Y^{(s)}\|_2 < T$$ divide by current value of Y^({s}). Make criteria invariate to a scale change, multiplyng by 20 doesnt change convergence criteria. Frobenius norm? L2 norm? 

To assess the quality of the imputation, Leave-One-Out Cross Validation (LOOCV) is used to generate a prediction error. LOOCV requires taking an observed value, setting it to NA (missing), and then performing the described imputation process. The prediction error can then be calculated as some function of the difference between the imputed value and the true value, $\hat y_{i,j} - y_{i,j}$.

Least squares estimate with gaussian error model, for the same variance assumption between cells.

What would a good statistical model be for variable frequency

AMMI Model, $$y_{ijk}`sim .mu + a_i + b_j + u_i^TDv_j+\sigma_{ij}\epsilon_{ij}$$ epsilon is standard normal so it turns the whole term to having variance i,j

```{r,eval=FALSE}
#matrix parameters
n_Sport = 20
n_Dport = 20

#get freqs
Sport_table = as.data.frame(table(argus$Sport))
Sport_table = Sport_table[order(-Sport_table$Freq),]
top_Sport = (head(Sport_table$Var1, n_Sport))

#get freqs
Dport_table = as.data.frame(table(argus$Dport))
Dport_table = Dport_table[order(-Dport_table$Freq),]
top_Dport = (head(Dport_table$Var1, n_Dport))

#create starting matrices
ports_combo_matrix = matrix(list(), nrow = n_Sport, ncol = n_Dport)
dimnames(ports_combo_matrix) = list(top_Sport, top_Dport)

ports_freq_matrix = matrix(0, nrow = n_Sport, ncol = n_Dport)
dimnames(ports_freq_matrix) = list(top_Sport, top_Dport)

nscore = function(x) {
  nscore = qqnorm(x, plot.it = FALSE)$x  # normal score 
  trn.table = data.frame(x=sort(x),nscore=sort(nscore))
  return (list(nscore=nscore, trn.table=trn.table))
}

#fill the ports_combo_matrix and ports_freq_matrix
for (s in 1:n_Sport){
  for (d in 1:n_Dport){
    combination = argus[is.element(argus$Sport, top_Sport[s])
                        & is.element(argus$Dport, top_Dport[d]),]
    obs = combination$SrcBytes
    n_obs = length(obs) #ignores NA values
    if (n_obs > 0){
      #obs = nscore(obs)$nscore #normal transformation
      for (i in 1:n_obs){
        ports_combo_matrix[[s,d]] = c(ports_combo_matrix[[s,d]],obs[i]) 
        #O(1) time to append values to a list?
        ports_freq_matrix[s,d] = ports_freq_matrix[s,d] + 1
      }
    }
  }
}

#create mean and variance matrix
ports_mean_matrix = matrix(NA, nrow = n_Sport, ncol = n_Dport)
dimnames(ports_mean_matrix) = list(top_Sport, top_Dport)

ports_variance_matrix = matrix(NA, nrow = n_Sport, ncol = n_Dport)
dimnames(ports_variance_matrix) = list(top_Sport, top_Dport)

#fill mean and variance matrix
for (s in 1:n_Sport){
  for (d in 1:n_Dport){
    if (ports_freq_matrix[s,d] == 1){
      ports_mean_matrix[s,d] = ports_combo_matrix[[s,d]]
      ports_variance_matrix[s,d] = 0
    }
    else if (ports_freq_matrix[s,d] > 1){
      ports_mean_matrix[s,d] = mean(ports_combo_matrix[[s,d]])
      ports_variance_matrix[s,d] = var(ports_combo_matrix[[s,d]])
    }
  }
}

#untuned ALS using softimpute
fit = softImpute(ports_mean_matrix,rank.max=2,lambda=0.9,trace=TRUE,type="als")
fit$d
filled = complete(ports_mean_matrix, fit)
plot(ports_mean_matrix[!is.na(ports_mean_matrix)], filled[!is.na(ports_mean_matrix)])
plot(ports_mean_matrix[!is.na(ports_mean_matrix)], filled[!is.na(ports_mean_matrix)], 
     xlim = c(0,5000), ylim = c(0,5000))


####Eckhart Young Theorem Implementation, Best Rank k Approximation####
S = 1000
k = 2
Y = ports_mean_matrix

#calculate overall mean
n = 0
sum = 0
for (s in 1:n_Sport){
  for (d in 1:n_Dport){
    if (ports_freq_matrix[s,d] != 0){
      sum = sum + ports_mean_matrix[s,d]
      n = n + ports_freq_matrix[s,d]
    }
  }
}
overall_mean = sum/n
#calculate row means and col means
row_means = rowMeans(ports_mean_matrix, na.rm = TRUE)
col_means = colMeans(ports_mean_matrix, na.rm = TRUE)

#Fill in missing values in Y with ANOVA
for (s in 1:n_Sport){
  for (d in 1:n_Dport){
    if (ports_freq_matrix[s,d] == 0){
      Y[s,d] = row_means[s] + col_means[d] - overall_mean
    }
  }
}

for (i in 1:S){
  #extract SVD
  svd_Y = svd(Y)
  D = diag(svd_Y$d)
  U = svd_Y$u
  V = svd_Y$v
  #EYM theorem
  EYM = U[,1:k] %*% D[,1:k] %*% V[,1:k]
  #replace imputed values with new values 
  for (s in 1:n_Sport){
    for (d in 1:n_Dport){
      if (ports_freq_matrix[s,d] == 0){
        Y[s,d] = EYM[s,d]
      }
    }
  }
}
```
